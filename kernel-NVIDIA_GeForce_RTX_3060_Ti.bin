//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33538619
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_86, texmode_independent
.address_size 64

	// .globl	composeRGBPixel
// kernel_HistogramRectAllChannelsReduction_$_localHist has been demoted
// kernel_HistogramRectOneChannelReduction_$_localHist has been demoted

.entry composeRGBPixel(
	.param .u64 .ptr .global .align 4 composeRGBPixel_param_0,
	.param .u32 composeRGBPixel_param_1,
	.param .u32 composeRGBPixel_param_2,
	.param .u32 composeRGBPixel_param_3,
	.param .u64 .ptr .global .align 4 composeRGBPixel_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [composeRGBPixel_param_0];
	ld.param.u32 	%r3, [composeRGBPixel_param_1];
	ld.param.u32 	%r5, [composeRGBPixel_param_2];
	ld.param.u32 	%r4, [composeRGBPixel_param_3];
	ld.param.u64 	%rd2, [composeRGBPixel_param_4];
	mov.b32 	%r6, %envreg4;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %tid.y;
	add.s32 	%r10, %r9, %r6;
	mad.lo.s32 	%r1, %r8, %r7, %r10;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mov.b32 	%r14, %envreg3;
	add.s32 	%r15, %r13, %r14;
	mad.lo.s32 	%r2, %r12, %r11, %r15;
	setp.ge.s32 	%p1, %r1, %r5;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_2;

	mad.lo.s32 	%r16, %r1, %r3, %r2;
	mul.wide.s32 	%rd3, %r16, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r17, [%rd4];
	shl.b32 	%r18, %r17, 24;
	shl.b32 	%r19, %r17, 8;
	and.b32  	%r20, %r19, 16711680;
	or.b32  	%r21, %r20, %r18;
	shr.u32 	%r22, %r17, 8;
	and.b32  	%r23, %r22, 65280;
	or.b32  	%r24, %r21, %r23;
	mad.lo.s32 	%r25, %r1, %r4, %r2;
	mul.wide.s32 	%rd5, %r25, 4;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.u32 	[%rd6], %r24;

$L__BB0_2:
	ret;

}
	// .globl	pixSubtract_inplace
.entry pixSubtract_inplace(
	.param .u64 .ptr .global .align 4 pixSubtract_inplace_param_0,
	.param .u64 .ptr .global .align 4 pixSubtract_inplace_param_1,
	.param .u32 pixSubtract_inplace_param_2,
	.param .u32 pixSubtract_inplace_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [pixSubtract_inplace_param_0];
	ld.param.u64 	%rd2, [pixSubtract_inplace_param_1];
	ld.param.u32 	%r2, [pixSubtract_inplace_param_2];
	ld.param.u32 	%r3, [pixSubtract_inplace_param_3];
	mov.b32 	%r4, %envreg4;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %tid.y;
	add.s32 	%r8, %r7, %r4;
	mad.lo.s32 	%r9, %r6, %r5, %r8;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mov.b32 	%r13, %envreg3;
	add.s32 	%r14, %r12, %r13;
	mad.lo.s32 	%r15, %r11, %r10, %r14;
	mad.lo.s32 	%r1, %r9, %r2, %r15;
	setp.ge.u32 	%p1, %r9, %r3;
	setp.ge.u32 	%p2, %r15, %r2;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB1_2;

	mul.wide.u32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u32 	%r16, [%rd4];
	not.b32 	%r17, %r16;
	add.s64 	%rd5, %rd1, %rd3;
	ld.global.u32 	%r18, [%rd5];
	and.b32  	%r19, %r18, %r17;
	st.global.u32 	[%rd5], %r19;

$L__BB1_2:
	ret;

}
	// .globl	morphoDilateHor_5x5
.entry morphoDilateHor_5x5(
	.param .u64 .ptr .global .align 4 morphoDilateHor_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_5x5_param_1,
	.param .u32 morphoDilateHor_5x5_param_2,
	.param .u32 morphoDilateHor_5x5_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoDilateHor_5x5_param_0];
	ld.param.u64 	%rd4, [morphoDilateHor_5x5_param_1];
	ld.param.u32 	%r8, [morphoDilateHor_5x5_param_2];
	ld.param.u32 	%r9, [morphoDilateHor_5x5_param_3];
	mov.b32 	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r14, %r13, %r10;
	mad.lo.s32 	%r1, %r12, %r11, %r14;
	rem.u32 	%r2, %r1, %r8;
	mul.lo.s32 	%r15, %r9, %r8;
	setp.le.u32 	%p1, %r15, %r1;
	@%p1 bra 	$L__BB2_6;

	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32 	%p2, %r2, 0;
	mov.u32 	%r36, 0;
	mov.u32 	%r35, %r36;
	@%p2 bra 	$L__BB2_3;

	ld.global.u32 	%r35, [%rd2+-4];

$L__BB2_3:
	add.s32 	%r18, %r8, -1;
	setp.eq.s32 	%p3, %r2, %r18;
	@%p3 bra 	$L__BB2_5;

	ld.global.u32 	%r36, [%rd2+4];

$L__BB2_5:
	shl.b32 	%r19, %r3, 2;
	or.b32  	%r20, %r19, %r3;
	shr.u32 	%r21, %r3, 2;
	or.b32  	%r22, %r20, %r21;
	shr.u32 	%r23, %r3, 1;
	or.b32  	%r24, %r22, %r23;
	shl.b32 	%r25, %r3, 1;
	or.b32  	%r26, %r24, %r25;
	shl.b32 	%r27, %r35, 30;
	or.b32  	%r28, %r26, %r27;
	shl.b32 	%r29, %r35, 31;
	or.b32  	%r30, %r28, %r29;
	shr.u32 	%r31, %r36, 30;
	or.b32  	%r32, %r30, %r31;
	shr.u32 	%r33, %r36, 31;
	or.b32  	%r34, %r32, %r33;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r34;

$L__BB2_6:
	ret;

}
	// .globl	morphoDilateVer_5x5
.entry morphoDilateVer_5x5(
	.param .u64 .ptr .global .align 4 morphoDilateVer_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateVer_5x5_param_1,
	.param .u32 morphoDilateVer_5x5_param_2,
	.param .u32 morphoDilateVer_5x5_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd1, [morphoDilateVer_5x5_param_0];
	ld.param.u64 	%rd2, [morphoDilateVer_5x5_param_1];
	ld.param.u32 	%r3, [morphoDilateVer_5x5_param_2];
	ld.param.u32 	%r4, [morphoDilateVer_5x5_param_3];
	mov.b32 	%r5, %envreg3;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	add.s32 	%r9, %r8, %r5;
	mad.lo.s32 	%r1, %r7, %r6, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %tid.y;
	mov.b32 	%r13, %envreg4;
	add.s32 	%r14, %r12, %r13;
	mad.lo.s32 	%r2, %r11, %r10, %r14;
	setp.ge.s32 	%p1, %r2, %r4;
	setp.ge.s32 	%p2, %r1, %r3;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB3_2;

	mad.lo.s32 	%r15, %r2, %r3, %r1;
	mul.wide.u32 	%rd3, %r15, 4;
	add.s64 	%rd4, %rd1, %rd3;
	setp.lt.s32 	%p4, %r2, 2;
	add.s32 	%r16, %r2, -2;
	selp.b32 	%r17, %r2, %r16, %p4;
	mul.lo.s32 	%r18, %r17, %r3;
	cvt.s64.s32 	%rd5, %r18;
	cvt.s64.s32 	%rd6, %r1;
	add.s64 	%rd7, %rd5, %rd6;
	shl.b64 	%rd8, %rd7, 2;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.u32 	%r19, [%rd9];
	ld.global.u32 	%r20, [%rd4];
	or.b32  	%r21, %r19, %r20;
	setp.gt.s32 	%p5, %r2, 0;
	selp.b32 	%r22, -1, 0, %p5;
	add.s32 	%r23, %r2, %r22;
	mul.lo.s32 	%r24, %r23, %r3;
	cvt.s64.s32 	%rd10, %r24;
	add.s64 	%rd11, %rd10, %rd6;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.u32 	%r25, [%rd13];
	or.b32  	%r26, %r21, %r25;
	add.s32 	%r27, %r4, -1;
	setp.gt.s32 	%p6, %r27, %r2;
	selp.u32 	%r28, 1, 0, %p6;
	add.s32 	%r29, %r2, %r28;
	mul.lo.s32 	%r30, %r29, %r3;
	cvt.s64.s32 	%rd14, %r30;
	add.s64 	%rd15, %rd14, %rd6;
	shl.b64 	%rd16, %rd15, 2;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.u32 	%r31, [%rd17];
	or.b32  	%r32, %r26, %r31;
	add.s32 	%r33, %r4, -2;
	setp.gt.s32 	%p7, %r33, %r2;
	add.s32 	%r34, %r2, 2;
	selp.b32 	%r35, %r34, %r2, %p7;
	mul.lo.s32 	%r36, %r35, %r3;
	cvt.s64.s32 	%rd18, %r36;
	add.s64 	%rd19, %rd18, %rd6;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u32 	%r37, [%rd21];
	or.b32  	%r38, %r32, %r37;
	add.s64 	%rd22, %rd2, %rd3;
	st.global.u32 	[%rd22], %r38;

$L__BB3_2:
	ret;

}
	// .globl	morphoDilateHor
.entry morphoDilateHor(
	.param .u64 .ptr .global .align 4 morphoDilateHor_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_param_1,
	.param .u32 morphoDilateHor_param_2,
	.param .u32 morphoDilateHor_param_3,
	.param .u32 morphoDilateHor_param_4,
	.param .u32 morphoDilateHor_param_5
)
{
	.reg .pred 	%p<33>;
	.reg .b32 	%r<567>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd12, [morphoDilateHor_param_0];
	ld.param.u64 	%rd13, [morphoDilateHor_param_1];
	ld.param.u32 	%r99, [morphoDilateHor_param_2];
	ld.param.u32 	%r100, [morphoDilateHor_param_3];
	ld.param.u32 	%r101, [morphoDilateHor_param_4];
	ld.param.u32 	%r102, [morphoDilateHor_param_5];
	mov.b32 	%r103, %envreg3;
	mov.u32 	%r104, %ctaid.x;
	mov.u32 	%r105, %ntid.x;
	mul.lo.s32 	%r1, %r105, %r104;
	mov.u32 	%r106, %tid.x;
	add.s32 	%r2, %r106, %r103;
	add.s32 	%r3, %r2, %r1;
	mov.u32 	%r107, %ctaid.y;
	mov.u32 	%r108, %ntid.y;
	mov.u32 	%r109, %tid.y;
	mov.b32 	%r110, %envreg4;
	add.s32 	%r111, %r109, %r110;
	mad.lo.s32 	%r112, %r108, %r107, %r111;
	mul.lo.s32 	%r4, %r112, %r101;
	add.s32 	%r5, %r4, %r3;
	mul.lo.s32 	%r113, %r102, %r101;
	setp.ge.u32 	%p1, %r5, %r113;
	@%p1 bra 	$L__BB4_50;

	setp.lt.s32 	%p2, %r100, 1;
	setp.lt.s32 	%p3, %r99, 1;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	$L__BB4_50;

	cvt.u64.u32 	%rd1, %r5;
	mul.wide.u32 	%rd14, %r5, 4;
	add.s64 	%rd2, %rd12, %rd14;
	ld.global.u32 	%r6, [%rd2];
	and.b32  	%r7, %r100, 31;
	and.b32  	%r115, %r99, 31;
	setp.ne.s32 	%p5, %r115, 0;
	mov.u32 	%r528, 0;
	selp.b32 	%r8, %r115, 31, %p5;
	selp.u32 	%r9, 1, 0, %p5;
	shr.s32 	%r10, %r99, 5;
	add.s32 	%r11, %r10, %r9;
	sub.s32 	%r12, %r3, %r11;
	add.s32 	%r13, %r11, %r3;
	setp.eq.s32 	%p6, %r3, 0;
	mov.u32 	%r527, %r528;
	@%p6 bra 	$L__BB4_4;

	ld.global.u32 	%r527, [%rd2+-4];

$L__BB4_4:
	add.s32 	%r117, %r101, -1;
	setp.eq.s32 	%p7, %r117, %r3;
	@%p7 bra 	$L__BB4_6;

	ld.global.u32 	%r528, [%rd2+4];

$L__BB4_6:
	add.s32 	%r18, %r8, -1;
	and.b32  	%r541, %r8, 3;
	setp.eq.s32 	%p8, %r8, %r7;
	@%p8 bra 	$L__BB4_25;
	bra.uni 	$L__BB4_7;

$L__BB4_25:
	setp.lt.u32 	%p18, %r18, 3;
	mov.u32 	%r547, 1;
	mov.u32 	%r553, %r6;
	@%p18 bra 	$L__BB4_28;

	sub.s32 	%r545, %r7, %r541;
	mov.u32 	%r547, 1;
	mov.u32 	%r553, %r6;

$L__BB4_27:
	neg.s32 	%r190, %r547;
	and.b32  	%r191, %r190, 31;
	shl.b32 	%r192, %r527, %r191;
	and.b32  	%r193, %r547, 31;
	shr.u32 	%r194, %r6, %r193;
	or.b32  	%r195, %r192, %r194;
	shl.b32 	%r196, %r6, %r193;
	shr.u32 	%r197, %r528, %r191;
	or.b32  	%r198, %r197, %r553;
	or.b32  	%r199, %r198, %r196;
	or.b32  	%r200, %r199, %r195;
	xor.b32  	%r201, %r193, 31;
	shl.b32 	%r202, %r527, %r201;
	add.s32 	%r203, %r547, 1;
	and.b32  	%r204, %r203, 31;
	shr.u32 	%r205, %r6, %r204;
	or.b32  	%r206, %r202, %r205;
	shl.b32 	%r207, %r6, %r204;
	shr.u32 	%r208, %r528, %r201;
	or.b32  	%r209, %r208, %r200;
	or.b32  	%r210, %r209, %r207;
	or.b32  	%r211, %r210, %r206;
	mov.u32 	%r212, 30;
	sub.s32 	%r213, %r212, %r547;
	and.b32  	%r214, %r213, 31;
	shl.b32 	%r215, %r527, %r214;
	add.s32 	%r216, %r547, 2;
	and.b32  	%r217, %r216, 31;
	shr.u32 	%r218, %r6, %r217;
	or.b32  	%r219, %r215, %r218;
	shl.b32 	%r220, %r6, %r217;
	shr.u32 	%r221, %r528, %r214;
	or.b32  	%r222, %r221, %r211;
	or.b32  	%r223, %r222, %r220;
	or.b32  	%r224, %r223, %r219;
	mov.u32 	%r225, 29;
	sub.s32 	%r226, %r225, %r547;
	and.b32  	%r227, %r226, 31;
	shl.b32 	%r228, %r527, %r227;
	add.s32 	%r229, %r547, 3;
	and.b32  	%r230, %r229, 31;
	shr.u32 	%r231, %r6, %r230;
	or.b32  	%r232, %r228, %r231;
	shl.b32 	%r233, %r6, %r230;
	shr.u32 	%r234, %r528, %r227;
	or.b32  	%r235, %r234, %r224;
	or.b32  	%r236, %r235, %r233;
	or.b32  	%r553, %r236, %r232;
	add.s32 	%r547, %r547, 4;
	add.s32 	%r545, %r545, -4;
	setp.ne.s32 	%p19, %r545, 0;
	@%p19 bra 	$L__BB4_27;

$L__BB4_28:
	setp.eq.s32 	%p20, %r541, 0;
	@%p20 bra 	$L__BB4_31;

	neg.s32 	%r549, %r547;

$L__BB4_30:
	.pragma "nounroll";
	and.b32  	%r237, %r549, 31;
	shl.b32 	%r238, %r527, %r237;
	and.b32  	%r239, %r547, 31;
	shr.u32 	%r240, %r6, %r239;
	or.b32  	%r241, %r238, %r240;
	shl.b32 	%r242, %r6, %r239;
	shr.u32 	%r243, %r528, %r237;
	or.b32  	%r244, %r243, %r553;
	or.b32  	%r245, %r244, %r242;
	or.b32  	%r553, %r245, %r241;
	add.s32 	%r547, %r547, 1;
	add.s32 	%r549, %r549, -1;
	add.s32 	%r541, %r541, -1;
	setp.ne.s32 	%p21, %r541, 0;
	@%p21 bra 	$L__BB4_30;
	bra.uni 	$L__BB4_31;

$L__BB4_7:
	setp.lt.u32 	%p9, %r18, 3;
	mov.u32 	%r536, 1;
	mov.u32 	%r553, %r6;
	@%p9 bra 	$L__BB4_19;

	sub.s32 	%r531, %r8, %r541;
	mov.u32 	%r536, 1;
	mov.u32 	%r553, %r6;

$L__BB4_9:
	setp.eq.s32 	%p10, %r536, %r8;
	@%p10 bra 	$L__BB4_11;
	bra.uni 	$L__BB4_10;

$L__BB4_11:
	mov.u32 	%r532, 0;
	bra.uni 	$L__BB4_12;

$L__BB4_10:
	neg.s32 	%r121, %r536;
	and.b32  	%r122, %r121, 31;
	shl.b32 	%r123, %r527, %r122;
	and.b32  	%r124, %r536, 31;
	shr.u32 	%r125, %r6, %r124;
	or.b32  	%r532, %r123, %r125;

$L__BB4_12:
	neg.s32 	%r127, %r536;
	and.b32  	%r128, %r127, 31;
	and.b32  	%r129, %r536, 31;
	shl.b32 	%r130, %r6, %r129;
	shr.u32 	%r131, %r528, %r128;
	or.b32  	%r132, %r131, %r553;
	or.b32  	%r133, %r132, %r130;
	or.b32  	%r134, %r133, %r532;
	add.s32 	%r135, %r536, 1;
	setp.eq.s32 	%p11, %r135, %r8;
	xor.b32  	%r136, %r129, 31;
	shl.b32 	%r137, %r527, %r136;
	and.b32  	%r138, %r135, 31;
	shr.u32 	%r139, %r6, %r138;
	or.b32  	%r140, %r137, %r139;
	selp.b32 	%r141, 0, %r140, %p11;
	shl.b32 	%r142, %r6, %r138;
	shr.u32 	%r143, %r528, %r136;
	or.b32  	%r144, %r143, %r134;
	or.b32  	%r145, %r144, %r142;
	or.b32  	%r26, %r145, %r141;
	add.s32 	%r27, %r536, 2;
	setp.eq.s32 	%p12, %r27, %r8;
	@%p12 bra 	$L__BB4_14;
	bra.uni 	$L__BB4_13;

$L__BB4_14:
	mov.u32 	%r533, 0;
	bra.uni 	$L__BB4_15;

$L__BB4_13:
	mov.u32 	%r146, 30;
	sub.s32 	%r147, %r146, %r536;
	and.b32  	%r148, %r147, 31;
	shl.b32 	%r149, %r527, %r148;
	and.b32  	%r150, %r27, 31;
	shr.u32 	%r151, %r6, %r150;
	or.b32  	%r533, %r149, %r151;

$L__BB4_15:
	mov.u32 	%r153, 30;
	sub.s32 	%r154, %r153, %r536;
	and.b32  	%r155, %r154, 31;
	and.b32  	%r156, %r27, 31;
	shl.b32 	%r157, %r6, %r156;
	shr.u32 	%r158, %r528, %r155;
	or.b32  	%r159, %r158, %r26;
	or.b32  	%r160, %r159, %r157;
	or.b32  	%r30, %r160, %r533;
	add.s32 	%r31, %r536, 3;
	setp.eq.s32 	%p13, %r31, %r8;
	@%p13 bra 	$L__BB4_17;
	bra.uni 	$L__BB4_16;

$L__BB4_17:
	mov.u32 	%r534, 0;
	bra.uni 	$L__BB4_18;

$L__BB4_16:
	mov.u32 	%r161, 29;
	sub.s32 	%r162, %r161, %r536;
	and.b32  	%r163, %r162, 31;
	shl.b32 	%r164, %r527, %r163;
	and.b32  	%r165, %r31, 31;
	shr.u32 	%r166, %r6, %r165;
	or.b32  	%r534, %r164, %r166;

$L__BB4_18:
	mov.u32 	%r168, 29;
	sub.s32 	%r169, %r168, %r536;
	and.b32  	%r170, %r169, 31;
	and.b32  	%r171, %r31, 31;
	shl.b32 	%r172, %r6, %r171;
	shr.u32 	%r173, %r528, %r170;
	or.b32  	%r174, %r173, %r30;
	or.b32  	%r175, %r174, %r172;
	or.b32  	%r553, %r175, %r534;
	add.s32 	%r536, %r536, 4;
	add.s32 	%r531, %r531, -4;
	setp.ne.s32 	%p14, %r531, 0;
	@%p14 bra 	$L__BB4_9;

$L__BB4_19:
	setp.eq.s32 	%p15, %r541, 0;
	@%p15 bra 	$L__BB4_31;

	sub.s32 	%r538, %r536, %r8;

$L__BB4_21:
	.pragma "nounroll";
	neg.s32 	%r45, %r536;
	setp.eq.s32 	%p16, %r538, 0;
	@%p16 bra 	$L__BB4_23;

	and.b32  	%r176, %r45, 31;
	shl.b32 	%r177, %r527, %r176;
	and.b32  	%r178, %r536, 31;
	shr.u32 	%r179, %r6, %r178;
	or.b32  	%r542, %r177, %r179;
	bra.uni 	$L__BB4_24;

$L__BB4_23:
	mov.u32 	%r542, 0;

$L__BB4_24:
	and.b32  	%r181, %r536, 31;
	shl.b32 	%r182, %r6, %r181;
	and.b32  	%r183, %r45, 31;
	shr.u32 	%r184, %r528, %r183;
	or.b32  	%r185, %r184, %r553;
	or.b32  	%r186, %r185, %r182;
	or.b32  	%r553, %r186, %r542;
	add.s32 	%r536, %r536, 1;
	add.s32 	%r538, %r538, 1;
	add.s32 	%r541, %r541, -1;
	setp.eq.s32 	%p17, %r541, 0;
	@%p17 bra 	$L__BB4_31;
	bra.uni 	$L__BB4_21;

$L__BB4_31:
	shl.b64 	%rd15, %rd1, 2;
	add.s64 	%rd3, %rd13, %rd15;
	setp.eq.s32 	%p22, %r11, 1;
	@%p22 bra 	$L__BB4_49;
	bra.uni 	$L__BB4_32;

$L__BB4_49:
	setp.eq.s32 	%p31, %r100, 32;
	selp.b32 	%r523, %r527, 0, %p31;
	setp.eq.s32 	%p32, %r99, 32;
	selp.b32 	%r524, %r528, 0, %p32;
	or.b32  	%r525, %r524, %r523;
	or.b32  	%r526, %r525, %r553;
	st.global.u32 	[%rd3], %r526;
	bra.uni 	$L__BB4_50;

$L__BB4_32:
	setp.lt.s32 	%p23, %r12, 0;
	mov.u32 	%r560, 0;
	mov.u32 	%r559, %r560;
	@%p23 bra 	$L__BB4_34;

	cvt.s64.s32 	%rd16, %r4;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd16;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd12, %rd19;
	ld.global.u32 	%r559, [%rd20];

$L__BB4_34:
	setp.ge.s32 	%p24, %r13, %r101;
	@%p24 bra 	$L__BB4_36;

	cvt.s64.s32 	%rd21, %r4;
	cvt.s64.s32 	%rd22, %r13;
	add.s64 	%rd23, %rd22, %rd21;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd25, %rd12, %rd24;
	ld.global.u32 	%r560, [%rd25];

$L__BB4_36:
	setp.lt.u32 	%p25, %r11, 2;
	@%p25 bra 	$L__BB4_48;

	neg.s32 	%r249, %r100;
	and.b32  	%r76, %r249, 31;
	neg.s32 	%r250, %r8;
	and.b32  	%r77, %r250, 31;
	add.s32 	%r251, %r2, %r10;
	add.s32 	%r252, %r251, %r1;
	add.s32 	%r253, %r252, %r9;
	add.s32 	%r557, %r253, -1;
	add.s32 	%r254, %r3, 1;
	mov.u32 	%r558, 1;
	sub.s32 	%r255, %r254, %r9;
	sub.s32 	%r556, %r255, %r10;
	cvt.s64.s32 	%rd26, %r13;
	cvt.s64.s32 	%rd27, %r4;
	add.s64 	%rd28, %rd27, %rd26;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd12, %rd29;
	add.s64 	%rd36, %rd30, -4;
	cvt.s64.s32 	%rd31, %r12;
	add.s64 	%rd32, %rd27, %rd31;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd34, %rd12, %rd33;
	add.s64 	%rd35, %rd34, 8;

$L__BB4_38:
	add.s64 	%rd8, %rd35, -4;
	setp.lt.s32 	%p26, %r556, 0;
	mov.u32 	%r563, 0;
	mov.u32 	%r562, %r563;
	@%p26 bra 	$L__BB4_40;

	ld.global.u32 	%r562, [%rd8];

$L__BB4_40:
	setp.lt.s32 	%p27, %r556, -1;
	@%p27 bra 	$L__BB4_42;

	ld.global.u32 	%r563, [%rd8+4];

$L__BB4_42:
	add.s64 	%rd36, %rd36, -4;
	setp.ge.s32 	%p28, %r557, %r101;
	mov.u32 	%r564, 0;
	@%p28 bra 	$L__BB4_44;

	ld.global.u32 	%r564, [%rd36+4];

$L__BB4_44:
	add.s32 	%r557, %r557, -1;
	setp.lt.s32 	%p29, %r557, %r101;
	@%p29 bra 	$L__BB4_46;
	bra.uni 	$L__BB4_45;

$L__BB4_46:
	ld.global.u32 	%r565, [%rd36];
	bra.uni 	$L__BB4_47;

$L__BB4_45:
	mov.u32 	%r565, 0;

$L__BB4_47:
	shl.b32 	%r260, %r559, %r76;
	shr.u32 	%r261, %r562, %r7;
	or.b32  	%r262, %r261, %r260;
	shr.u32 	%r263, %r563, %r7;
	shl.b32 	%r264, %r562, %r76;
	or.b32  	%r265, %r263, %r264;
	shl.b32 	%r266, %r564, %r8;
	shr.u32 	%r267, %r560, %r77;
	or.b32  	%r268, %r266, %r267;
	shr.u32 	%r269, %r564, %r77;
	shl.b32 	%r270, %r565, %r8;
	or.b32  	%r271, %r270, %r269;
	shl.b32 	%r272, %r271, 1;
	shr.u32 	%r273, %r268, 31;
	or.b32  	%r274, %r273, %r553;
	or.b32  	%r275, %r274, %r272;
	shr.u32 	%r276, %r265, 31;
	or.b32  	%r277, %r275, %r276;
	shl.b32 	%r278, %r262, 1;
	or.b32  	%r279, %r277, %r278;
	shl.b32 	%r280, %r271, 2;
	shr.u32 	%r281, %r268, 30;
	or.b32  	%r282, %r281, %r279;
	or.b32  	%r283, %r282, %r280;
	shr.u32 	%r284, %r265, 30;
	or.b32  	%r285, %r283, %r284;
	shl.b32 	%r286, %r262, 2;
	or.b32  	%r287, %r285, %r286;
	shl.b32 	%r288, %r271, 3;
	shr.u32 	%r289, %r268, 29;
	or.b32  	%r290, %r289, %r287;
	or.b32  	%r291, %r290, %r288;
	shr.u32 	%r292, %r265, 29;
	or.b32  	%r293, %r291, %r292;
	shl.b32 	%r294, %r262, 3;
	or.b32  	%r295, %r293, %r294;
	shl.b32 	%r296, %r271, 4;
	shr.u32 	%r297, %r268, 28;
	or.b32  	%r298, %r297, %r295;
	or.b32  	%r299, %r298, %r296;
	shr.u32 	%r300, %r265, 28;
	or.b32  	%r301, %r299, %r300;
	shl.b32 	%r302, %r262, 4;
	or.b32  	%r303, %r301, %r302;
	shl.b32 	%r304, %r271, 5;
	shr.u32 	%r305, %r268, 27;
	or.b32  	%r306, %r305, %r303;
	or.b32  	%r307, %r306, %r304;
	shr.u32 	%r308, %r265, 27;
	or.b32  	%r309, %r307, %r308;
	shl.b32 	%r310, %r262, 5;
	or.b32  	%r311, %r309, %r310;
	shl.b32 	%r312, %r271, 6;
	shr.u32 	%r313, %r268, 26;
	or.b32  	%r314, %r313, %r311;
	or.b32  	%r315, %r314, %r312;
	shr.u32 	%r316, %r265, 26;
	or.b32  	%r317, %r315, %r316;
	shl.b32 	%r318, %r262, 6;
	or.b32  	%r319, %r317, %r318;
	shl.b32 	%r320, %r271, 7;
	shr.u32 	%r321, %r268, 25;
	or.b32  	%r322, %r321, %r319;
	or.b32  	%r323, %r322, %r320;
	shr.u32 	%r324, %r265, 25;
	or.b32  	%r325, %r323, %r324;
	shl.b32 	%r326, %r262, 7;
	or.b32  	%r327, %r325, %r326;
	shl.b32 	%r328, %r271, 8;
	shr.u32 	%r329, %r268, 24;
	or.b32  	%r330, %r329, %r327;
	or.b32  	%r331, %r330, %r328;
	shr.u32 	%r332, %r265, 24;
	or.b32  	%r333, %r331, %r332;
	shl.b32 	%r334, %r262, 8;
	or.b32  	%r335, %r333, %r334;
	shl.b32 	%r336, %r271, 9;
	shr.u32 	%r337, %r268, 23;
	or.b32  	%r338, %r337, %r335;
	or.b32  	%r339, %r338, %r336;
	shr.u32 	%r340, %r265, 23;
	or.b32  	%r341, %r339, %r340;
	shl.b32 	%r342, %r262, 9;
	or.b32  	%r343, %r341, %r342;
	shl.b32 	%r344, %r271, 10;
	shr.u32 	%r345, %r268, 22;
	or.b32  	%r346, %r345, %r343;
	or.b32  	%r347, %r346, %r344;
	shr.u32 	%r348, %r265, 22;
	or.b32  	%r349, %r347, %r348;
	shl.b32 	%r350, %r262, 10;
	or.b32  	%r351, %r349, %r350;
	shl.b32 	%r352, %r271, 11;
	shr.u32 	%r353, %r268, 21;
	or.b32  	%r354, %r353, %r351;
	or.b32  	%r355, %r354, %r352;
	shr.u32 	%r356, %r265, 21;
	or.b32  	%r357, %r355, %r356;
	shl.b32 	%r358, %r262, 11;
	or.b32  	%r359, %r357, %r358;
	shl.b32 	%r360, %r271, 12;
	shr.u32 	%r361, %r268, 20;
	or.b32  	%r362, %r361, %r359;
	or.b32  	%r363, %r362, %r360;
	shr.u32 	%r364, %r265, 20;
	or.b32  	%r365, %r363, %r364;
	shl.b32 	%r366, %r262, 12;
	or.b32  	%r367, %r365, %r366;
	shl.b32 	%r368, %r271, 13;
	shr.u32 	%r369, %r268, 19;
	or.b32  	%r370, %r369, %r367;
	or.b32  	%r371, %r370, %r368;
	shr.u32 	%r372, %r265, 19;
	or.b32  	%r373, %r371, %r372;
	shl.b32 	%r374, %r262, 13;
	or.b32  	%r375, %r373, %r374;
	shl.b32 	%r376, %r271, 14;
	shr.u32 	%r377, %r268, 18;
	or.b32  	%r378, %r377, %r375;
	or.b32  	%r379, %r378, %r376;
	shr.u32 	%r380, %r265, 18;
	or.b32  	%r381, %r379, %r380;
	shl.b32 	%r382, %r262, 14;
	or.b32  	%r383, %r381, %r382;
	shl.b32 	%r384, %r271, 15;
	shr.u32 	%r385, %r268, 17;
	or.b32  	%r386, %r385, %r383;
	or.b32  	%r387, %r386, %r384;
	shr.u32 	%r388, %r265, 17;
	or.b32  	%r389, %r387, %r388;
	shl.b32 	%r390, %r262, 15;
	or.b32  	%r391, %r389, %r390;
	shl.b32 	%r392, %r271, 16;
	shr.u32 	%r393, %r268, 16;
	or.b32  	%r394, %r393, %r391;
	or.b32  	%r395, %r394, %r392;
	shr.u32 	%r396, %r265, 16;
	or.b32  	%r397, %r395, %r396;
	shl.b32 	%r398, %r262, 16;
	or.b32  	%r399, %r397, %r398;
	shl.b32 	%r400, %r271, 17;
	shr.u32 	%r401, %r268, 15;
	or.b32  	%r402, %r401, %r399;
	or.b32  	%r403, %r402, %r400;
	shr.u32 	%r404, %r265, 15;
	or.b32  	%r405, %r403, %r404;
	shl.b32 	%r406, %r262, 17;
	or.b32  	%r407, %r405, %r406;
	shl.b32 	%r408, %r271, 18;
	shr.u32 	%r409, %r268, 14;
	or.b32  	%r410, %r409, %r407;
	or.b32  	%r411, %r410, %r408;
	shr.u32 	%r412, %r265, 14;
	or.b32  	%r413, %r411, %r412;
	shl.b32 	%r414, %r262, 18;
	or.b32  	%r415, %r413, %r414;
	shl.b32 	%r416, %r271, 19;
	shr.u32 	%r417, %r268, 13;
	or.b32  	%r418, %r417, %r415;
	or.b32  	%r419, %r418, %r416;
	shr.u32 	%r420, %r265, 13;
	or.b32  	%r421, %r419, %r420;
	shl.b32 	%r422, %r262, 19;
	or.b32  	%r423, %r421, %r422;
	shl.b32 	%r424, %r271, 20;
	shr.u32 	%r425, %r268, 12;
	or.b32  	%r426, %r425, %r423;
	or.b32  	%r427, %r426, %r424;
	shr.u32 	%r428, %r265, 12;
	or.b32  	%r429, %r427, %r428;
	shl.b32 	%r430, %r262, 20;
	or.b32  	%r431, %r429, %r430;
	shl.b32 	%r432, %r271, 21;
	shr.u32 	%r433, %r268, 11;
	or.b32  	%r434, %r433, %r431;
	or.b32  	%r435, %r434, %r432;
	shr.u32 	%r436, %r265, 11;
	or.b32  	%r437, %r435, %r436;
	shl.b32 	%r438, %r262, 21;
	or.b32  	%r439, %r437, %r438;
	shl.b32 	%r440, %r271, 22;
	shr.u32 	%r441, %r268, 10;
	or.b32  	%r442, %r441, %r439;
	or.b32  	%r443, %r442, %r440;
	shr.u32 	%r444, %r265, 10;
	or.b32  	%r445, %r443, %r444;
	shl.b32 	%r446, %r262, 22;
	or.b32  	%r447, %r445, %r446;
	shl.b32 	%r448, %r271, 23;
	shr.u32 	%r449, %r268, 9;
	or.b32  	%r450, %r449, %r447;
	or.b32  	%r451, %r450, %r448;
	shr.u32 	%r452, %r265, 9;
	or.b32  	%r453, %r451, %r452;
	shl.b32 	%r454, %r262, 23;
	or.b32  	%r455, %r453, %r454;
	shl.b32 	%r456, %r271, 24;
	shr.u32 	%r457, %r268, 8;
	or.b32  	%r458, %r457, %r455;
	or.b32  	%r459, %r458, %r456;
	shr.u32 	%r460, %r265, 8;
	or.b32  	%r461, %r459, %r460;
	shl.b32 	%r462, %r262, 24;
	or.b32  	%r463, %r461, %r462;
	shl.b32 	%r464, %r271, 25;
	shr.u32 	%r465, %r268, 7;
	or.b32  	%r466, %r465, %r463;
	or.b32  	%r467, %r466, %r464;
	shr.u32 	%r468, %r265, 7;
	or.b32  	%r469, %r467, %r468;
	shl.b32 	%r470, %r262, 25;
	or.b32  	%r471, %r469, %r470;
	shl.b32 	%r472, %r271, 26;
	shr.u32 	%r473, %r268, 6;
	or.b32  	%r474, %r473, %r471;
	or.b32  	%r475, %r474, %r472;
	shr.u32 	%r476, %r265, 6;
	or.b32  	%r477, %r475, %r476;
	shl.b32 	%r478, %r262, 26;
	or.b32  	%r479, %r477, %r478;
	shl.b32 	%r480, %r271, 27;
	shr.u32 	%r481, %r268, 5;
	or.b32  	%r482, %r481, %r479;
	or.b32  	%r483, %r482, %r480;
	shr.u32 	%r484, %r265, 5;
	or.b32  	%r485, %r483, %r484;
	shl.b32 	%r486, %r262, 27;
	or.b32  	%r487, %r485, %r486;
	shl.b32 	%r488, %r271, 28;
	shr.u32 	%r489, %r268, 4;
	or.b32  	%r490, %r489, %r487;
	or.b32  	%r491, %r490, %r488;
	shr.u32 	%r492, %r265, 4;
	or.b32  	%r493, %r491, %r492;
	shl.b32 	%r494, %r262, 28;
	or.b32  	%r495, %r493, %r494;
	shl.b32 	%r496, %r271, 29;
	shr.u32 	%r497, %r268, 3;
	or.b32  	%r498, %r497, %r495;
	or.b32  	%r499, %r498, %r496;
	shr.u32 	%r500, %r265, 3;
	or.b32  	%r501, %r499, %r500;
	shl.b32 	%r502, %r262, 29;
	or.b32  	%r503, %r501, %r502;
	shl.b32 	%r504, %r271, 30;
	shr.u32 	%r505, %r268, 2;
	or.b32  	%r506, %r505, %r503;
	or.b32  	%r507, %r506, %r504;
	shr.u32 	%r508, %r265, 2;
	or.b32  	%r509, %r507, %r508;
	shl.b32 	%r510, %r262, 30;
	or.b32  	%r511, %r509, %r510;
	shl.b32 	%r512, %r271, 31;
	shr.u32 	%r513, %r268, 1;
	or.b32  	%r514, %r513, %r511;
	or.b32  	%r515, %r514, %r512;
	shr.u32 	%r516, %r265, 1;
	or.b32  	%r517, %r515, %r516;
	shl.b32 	%r518, %r262, 31;
	or.b32  	%r519, %r517, %r518;
	or.b32  	%r520, %r265, %r262;
	or.b32  	%r521, %r520, %r268;
	or.b32  	%r522, %r521, %r271;
	or.b32  	%r553, %r522, %r519;
	add.s32 	%r556, %r556, 1;
	add.s64 	%rd35, %rd35, 4;
	add.s32 	%r558, %r558, 1;
	setp.lt.u32 	%p30, %r558, %r11;
	mov.u32 	%r559, %r563;
	mov.u32 	%r560, %r565;
	@%p30 bra 	$L__BB4_38;

$L__BB4_48:
	st.global.u32 	[%rd3], %r553;

$L__BB4_50:
	ret;

}
	// .globl	morphoDilateHor_32word
.entry morphoDilateHor_32word(
	.param .u64 .ptr .global .align 4 morphoDilateHor_32word_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_32word_param_1,
	.param .u32 morphoDilateHor_32word_param_2,
	.param .u32 morphoDilateHor_32word_param_3,
	.param .u32 morphoDilateHor_32word_param_4,
	.param .u8 morphoDilateHor_32word_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<235>;
	.reg .b64 	%rd<8>;


	ld.param.s8 	%rs1, [morphoDilateHor_32word_param_5];
	ld.param.u64 	%rd3, [morphoDilateHor_32word_param_0];
	ld.param.u64 	%rd4, [morphoDilateHor_32word_param_1];
	ld.param.u32 	%r62, [morphoDilateHor_32word_param_2];
	ld.param.u32 	%r63, [morphoDilateHor_32word_param_3];
	ld.param.u32 	%r64, [morphoDilateHor_32word_param_4];
	mov.b32 	%r65, %envreg3;
	mov.u32 	%r66, %ctaid.x;
	mov.u32 	%r67, %ntid.x;
	mov.u32 	%r68, %tid.x;
	add.s32 	%r69, %r68, %r65;
	mad.lo.s32 	%r1, %r67, %r66, %r69;
	mov.u32 	%r70, %ctaid.y;
	mov.u32 	%r71, %ntid.y;
	mov.u32 	%r72, %tid.y;
	mov.b32 	%r73, %envreg4;
	add.s32 	%r74, %r72, %r73;
	mad.lo.s32 	%r75, %r71, %r70, %r74;
	mad.lo.s32 	%r2, %r75, %r63, %r1;
	mul.lo.s32 	%r76, %r64, %r63;
	setp.ge.u32 	%p1, %r2, %r76;
	@%p1 bra 	$L__BB5_32;

	cvt.u64.u32 	%rd1, %r2;
	mul.wide.u32 	%rd5, %r2, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32 	%p2, %r1, 0;
	mov.u32 	%r209, 0;
	mov.u32 	%r208, %r209;
	@%p2 bra 	$L__BB5_3;

	ld.global.u32 	%r208, [%rd2+-4];

$L__BB5_3:
	add.s32 	%r79, %r63, -1;
	setp.eq.s32 	%p3, %r79, %r1;
	@%p3 bra 	$L__BB5_5;

	ld.global.u32 	%r209, [%rd2+4];

$L__BB5_5:
	setp.lt.s32 	%p4, %r62, 1;
	mov.u32 	%r234, %r3;
	@%p4 bra 	$L__BB5_31;

	setp.eq.s16 	%p5, %rs1, 0;
	add.s32 	%r8, %r62, -1;
	and.b32  	%r222, %r62, 3;
	@%p5 bra 	$L__BB5_25;

	setp.lt.u32 	%p6, %r8, 3;
	mov.u32 	%r217, 1;
	mov.u32 	%r234, %r3;
	@%p6 bra 	$L__BB5_19;

	sub.s32 	%r212, %r62, %r222;
	mov.u32 	%r217, 1;
	mov.u32 	%r234, %r3;

$L__BB5_9:
	setp.eq.s32 	%p7, %r217, %r62;
	@%p7 bra 	$L__BB5_11;
	bra.uni 	$L__BB5_10;

$L__BB5_11:
	mov.u32 	%r213, 0;
	bra.uni 	$L__BB5_12;

$L__BB5_10:
	neg.s32 	%r83, %r217;
	and.b32  	%r84, %r83, 31;
	shl.b32 	%r85, %r208, %r84;
	and.b32  	%r86, %r217, 31;
	shr.u32 	%r87, %r3, %r86;
	or.b32  	%r213, %r85, %r87;

$L__BB5_12:
	neg.s32 	%r89, %r217;
	and.b32  	%r90, %r89, 31;
	and.b32  	%r91, %r217, 31;
	shl.b32 	%r92, %r3, %r91;
	shr.u32 	%r93, %r209, %r90;
	or.b32  	%r94, %r93, %r234;
	or.b32  	%r95, %r94, %r92;
	or.b32  	%r96, %r95, %r213;
	add.s32 	%r97, %r217, 1;
	setp.eq.s32 	%p8, %r97, %r62;
	xor.b32  	%r98, %r91, 31;
	shl.b32 	%r99, %r208, %r98;
	and.b32  	%r100, %r97, 31;
	shr.u32 	%r101, %r3, %r100;
	or.b32  	%r102, %r99, %r101;
	selp.b32 	%r103, 0, %r102, %p8;
	shl.b32 	%r104, %r3, %r100;
	shr.u32 	%r105, %r209, %r98;
	or.b32  	%r106, %r105, %r96;
	or.b32  	%r107, %r106, %r104;
	or.b32  	%r16, %r107, %r103;
	add.s32 	%r17, %r217, 2;
	setp.eq.s32 	%p9, %r17, %r62;
	@%p9 bra 	$L__BB5_14;
	bra.uni 	$L__BB5_13;

$L__BB5_14:
	mov.u32 	%r214, 0;
	bra.uni 	$L__BB5_15;

$L__BB5_13:
	mov.u32 	%r108, 30;
	sub.s32 	%r109, %r108, %r217;
	and.b32  	%r110, %r109, 31;
	shl.b32 	%r111, %r208, %r110;
	and.b32  	%r112, %r17, 31;
	shr.u32 	%r113, %r3, %r112;
	or.b32  	%r214, %r111, %r113;

$L__BB5_15:
	mov.u32 	%r115, 30;
	sub.s32 	%r116, %r115, %r217;
	and.b32  	%r117, %r116, 31;
	and.b32  	%r118, %r17, 31;
	shl.b32 	%r119, %r3, %r118;
	shr.u32 	%r120, %r209, %r117;
	or.b32  	%r121, %r120, %r16;
	or.b32  	%r122, %r121, %r119;
	or.b32  	%r20, %r122, %r214;
	add.s32 	%r21, %r217, 3;
	setp.eq.s32 	%p10, %r21, %r62;
	@%p10 bra 	$L__BB5_17;
	bra.uni 	$L__BB5_16;

$L__BB5_17:
	mov.u32 	%r215, 0;
	bra.uni 	$L__BB5_18;

$L__BB5_16:
	mov.u32 	%r123, 29;
	sub.s32 	%r124, %r123, %r217;
	and.b32  	%r125, %r124, 31;
	shl.b32 	%r126, %r208, %r125;
	and.b32  	%r127, %r21, 31;
	shr.u32 	%r128, %r3, %r127;
	or.b32  	%r215, %r126, %r128;

$L__BB5_18:
	mov.u32 	%r130, 29;
	sub.s32 	%r131, %r130, %r217;
	and.b32  	%r132, %r131, 31;
	and.b32  	%r133, %r21, 31;
	shl.b32 	%r134, %r3, %r133;
	shr.u32 	%r135, %r209, %r132;
	or.b32  	%r136, %r135, %r20;
	or.b32  	%r137, %r136, %r134;
	or.b32  	%r234, %r137, %r215;
	add.s32 	%r217, %r217, 4;
	add.s32 	%r212, %r212, -4;
	setp.ne.s32 	%p11, %r212, 0;
	@%p11 bra 	$L__BB5_9;

$L__BB5_19:
	setp.eq.s32 	%p12, %r222, 0;
	@%p12 bra 	$L__BB5_31;

	sub.s32 	%r219, %r217, %r62;

$L__BB5_21:
	.pragma "nounroll";
	neg.s32 	%r35, %r217;
	setp.eq.s32 	%p13, %r219, 0;
	@%p13 bra 	$L__BB5_23;

	and.b32  	%r138, %r35, 31;
	shl.b32 	%r139, %r208, %r138;
	and.b32  	%r140, %r217, 31;
	shr.u32 	%r141, %r3, %r140;
	or.b32  	%r223, %r139, %r141;
	bra.uni 	$L__BB5_24;

$L__BB5_23:
	mov.u32 	%r223, 0;

$L__BB5_24:
	and.b32  	%r143, %r217, 31;
	shl.b32 	%r144, %r3, %r143;
	and.b32  	%r145, %r35, 31;
	shr.u32 	%r146, %r209, %r145;
	or.b32  	%r147, %r146, %r234;
	or.b32  	%r148, %r147, %r144;
	or.b32  	%r234, %r148, %r223;
	add.s32 	%r217, %r217, 1;
	add.s32 	%r219, %r219, 1;
	add.s32 	%r222, %r222, -1;
	setp.eq.s32 	%p14, %r222, 0;
	@%p14 bra 	$L__BB5_31;
	bra.uni 	$L__BB5_21;

$L__BB5_25:
	setp.lt.u32 	%p15, %r8, 3;
	mov.u32 	%r228, 1;
	mov.u32 	%r234, %r3;
	@%p15 bra 	$L__BB5_28;

	sub.s32 	%r226, %r62, %r222;
	mov.u32 	%r228, 1;
	mov.u32 	%r234, %r3;

$L__BB5_27:
	neg.s32 	%r152, %r228;
	and.b32  	%r153, %r152, 31;
	shl.b32 	%r154, %r208, %r153;
	and.b32  	%r155, %r228, 31;
	shr.u32 	%r156, %r3, %r155;
	or.b32  	%r157, %r154, %r156;
	shl.b32 	%r158, %r3, %r155;
	shr.u32 	%r159, %r209, %r153;
	or.b32  	%r160, %r159, %r234;
	or.b32  	%r161, %r160, %r158;
	or.b32  	%r162, %r161, %r157;
	xor.b32  	%r163, %r155, 31;
	shl.b32 	%r164, %r208, %r163;
	add.s32 	%r165, %r228, 1;
	and.b32  	%r166, %r165, 31;
	shr.u32 	%r167, %r3, %r166;
	or.b32  	%r168, %r164, %r167;
	shl.b32 	%r169, %r3, %r166;
	shr.u32 	%r170, %r209, %r163;
	or.b32  	%r171, %r170, %r162;
	or.b32  	%r172, %r171, %r169;
	or.b32  	%r173, %r172, %r168;
	mov.u32 	%r174, 30;
	sub.s32 	%r175, %r174, %r228;
	and.b32  	%r176, %r175, 31;
	shl.b32 	%r177, %r208, %r176;
	add.s32 	%r178, %r228, 2;
	and.b32  	%r179, %r178, 31;
	shr.u32 	%r180, %r3, %r179;
	or.b32  	%r181, %r177, %r180;
	shl.b32 	%r182, %r3, %r179;
	shr.u32 	%r183, %r209, %r176;
	or.b32  	%r184, %r183, %r173;
	or.b32  	%r185, %r184, %r182;
	or.b32  	%r186, %r185, %r181;
	mov.u32 	%r187, 29;
	sub.s32 	%r188, %r187, %r228;
	and.b32  	%r189, %r188, 31;
	shl.b32 	%r190, %r208, %r189;
	add.s32 	%r191, %r228, 3;
	and.b32  	%r192, %r191, 31;
	shr.u32 	%r193, %r3, %r192;
	or.b32  	%r194, %r190, %r193;
	shl.b32 	%r195, %r3, %r192;
	shr.u32 	%r196, %r209, %r189;
	or.b32  	%r197, %r196, %r186;
	or.b32  	%r198, %r197, %r195;
	or.b32  	%r234, %r198, %r194;
	add.s32 	%r228, %r228, 4;
	add.s32 	%r226, %r226, -4;
	setp.ne.s32 	%p16, %r226, 0;
	@%p16 bra 	$L__BB5_27;

$L__BB5_28:
	setp.eq.s32 	%p17, %r222, 0;
	@%p17 bra 	$L__BB5_31;

	neg.s32 	%r230, %r228;

$L__BB5_30:
	.pragma "nounroll";
	and.b32  	%r199, %r230, 31;
	shl.b32 	%r200, %r208, %r199;
	and.b32  	%r201, %r228, 31;
	shr.u32 	%r202, %r3, %r201;
	or.b32  	%r203, %r200, %r202;
	shl.b32 	%r204, %r3, %r201;
	shr.u32 	%r205, %r209, %r199;
	or.b32  	%r206, %r205, %r234;
	or.b32  	%r207, %r206, %r204;
	or.b32  	%r234, %r207, %r203;
	add.s32 	%r228, %r228, 1;
	add.s32 	%r230, %r230, -1;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p18, %r222, 0;
	@%p18 bra 	$L__BB5_30;

$L__BB5_31:
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r234;

$L__BB5_32:
	ret;

}
	// .globl	morphoDilateVer
.entry morphoDilateVer(
	.param .u64 .ptr .global .align 4 morphoDilateVer_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateVer_param_1,
	.param .u32 morphoDilateVer_param_2,
	.param .u32 morphoDilateVer_param_3,
	.param .u32 morphoDilateVer_param_4,
	.param .u32 morphoDilateVer_param_5
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<64>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd11, [morphoDilateVer_param_0];
	ld.param.u64 	%rd12, [morphoDilateVer_param_1];
	ld.param.u32 	%r22, [morphoDilateVer_param_2];
	ld.param.u32 	%r23, [morphoDilateVer_param_3];
	ld.param.u32 	%r24, [morphoDilateVer_param_4];
	ld.param.u32 	%r25, [morphoDilateVer_param_5];
	mov.b32 	%r26, %envreg3;
	mov.u32 	%r27, %ctaid.x;
	mov.u32 	%r28, %ntid.x;
	mov.u32 	%r29, %tid.x;
	add.s32 	%r30, %r29, %r26;
	mad.lo.s32 	%r1, %r28, %r27, %r30;
	mov.u32 	%r31, %ctaid.y;
	mov.u32 	%r32, %ntid.y;
	mov.u32 	%r33, %tid.y;
	mov.b32 	%r34, %envreg4;
	add.s32 	%r35, %r33, %r34;
	mad.lo.s32 	%r2, %r32, %r31, %r35;
	setp.ge.s32 	%p1, %r2, %r24;
	setp.ge.s32 	%p2, %r1, %r23;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB6_9;

	mad.lo.s32 	%r36, %r2, %r23, %r1;
	cvt.u64.u32 	%rd1, %r36;
	mul.wide.u32 	%rd13, %r36, 4;
	add.s64 	%rd14, %rd11, %rd13;
	ld.global.u32 	%r63, [%rd14];
	sub.s32 	%r37, %r2, %r25;
	max.s32 	%r4, %r37, 0;
	sub.s32 	%r38, %r24, %r22;
	setp.gt.s32 	%p4, %r38, %r2;
	add.s32 	%r39, %r2, %r22;
	add.s32 	%r40, %r24, -1;
	selp.b32 	%r5, %r39, %r40, %p4;
	setp.lt.s32 	%p5, %r5, %r4;
	@%p5 bra 	$L__BB6_8;

	cvt.s64.s32 	%rd2, %r1;
	add.s32 	%r42, %r5, 1;
	sub.s32 	%r43, %r42, %r4;
	and.b32  	%r57, %r43, 3;
	setp.eq.s32 	%p6, %r57, 0;
	mov.u32 	%r58, %r4;
	@%p6 bra 	$L__BB6_5;

	mul.lo.s32 	%r44, %r23, %r4;
	cvt.s64.s32 	%rd15, %r44;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd26, %rd11, %rd17;
	mul.wide.s32 	%rd4, %r23, 4;
	mov.u32 	%r58, %r4;

$L__BB6_4:
	.pragma "nounroll";
	ld.global.u32 	%r45, [%rd26];
	or.b32  	%r63, %r45, %r63;
	add.s32 	%r58, %r58, 1;
	add.s64 	%rd26, %rd26, %rd4;
	add.s32 	%r57, %r57, -1;
	setp.ne.s32 	%p7, %r57, 0;
	@%p7 bra 	$L__BB6_4;

$L__BB6_5:
	sub.s32 	%r46, %r5, %r4;
	setp.lt.u32 	%p8, %r46, 3;
	@%p8 bra 	$L__BB6_8;

	add.s32 	%r61, %r58, -1;
	mul.lo.s32 	%r47, %r58, %r23;
	cvt.s64.s32 	%rd18, %r47;
	add.s64 	%rd19, %rd18, %rd2;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd27, %rd11, %rd20;
	mul.wide.s32 	%rd8, %r23, 4;

$L__BB6_7:
	ld.global.u32 	%r48, [%rd27];
	or.b32  	%r49, %r48, %r63;
	add.s64 	%rd21, %rd27, %rd8;
	ld.global.u32 	%r50, [%rd21];
	or.b32  	%r51, %r50, %r49;
	add.s64 	%rd22, %rd21, %rd8;
	ld.global.u32 	%r52, [%rd22];
	or.b32  	%r53, %r52, %r51;
	add.s64 	%rd23, %rd22, %rd8;
	add.s64 	%rd27, %rd23, %rd8;
	ld.global.u32 	%r54, [%rd23];
	or.b32  	%r63, %r54, %r53;
	add.s32 	%r61, %r61, 4;
	setp.lt.s32 	%p9, %r61, %r5;
	@%p9 bra 	$L__BB6_7;

$L__BB6_8:
	shl.b64 	%rd24, %rd1, 2;
	add.s64 	%rd25, %rd12, %rd24;
	st.global.u32 	[%rd25], %r63;

$L__BB6_9:
	ret;

}
	// .globl	morphoErodeHor_5x5
.entry morphoErodeHor_5x5(
	.param .u64 .ptr .global .align 4 morphoErodeHor_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_5x5_param_1,
	.param .u32 morphoErodeHor_5x5_param_2,
	.param .u32 morphoErodeHor_5x5_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoErodeHor_5x5_param_0];
	ld.param.u64 	%rd4, [morphoErodeHor_5x5_param_1];
	ld.param.u32 	%r8, [morphoErodeHor_5x5_param_2];
	ld.param.u32 	%r9, [morphoErodeHor_5x5_param_3];
	mov.b32 	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r14, %r13, %r10;
	mad.lo.s32 	%r1, %r12, %r11, %r14;
	rem.u32 	%r2, %r1, %r8;
	mul.lo.s32 	%r15, %r9, %r8;
	setp.le.u32 	%p1, %r15, %r1;
	@%p1 bra 	$L__BB7_6;

	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32 	%p2, %r2, 0;
	mov.u32 	%r36, -1;
	mov.u32 	%r35, %r36;
	@%p2 bra 	$L__BB7_3;

	ld.global.u32 	%r35, [%rd2+-4];

$L__BB7_3:
	add.s32 	%r18, %r8, -1;
	setp.eq.s32 	%p3, %r2, %r18;
	@%p3 bra 	$L__BB7_5;

	ld.global.u32 	%r36, [%rd2+4];

$L__BB7_5:
	shr.u32 	%r19, %r3, 1;
	shl.b32 	%r20, %r35, 31;
	or.b32  	%r21, %r20, %r19;
	and.b32  	%r22, %r21, %r3;
	shr.u32 	%r23, %r36, 31;
	shl.b32 	%r24, %r3, 1;
	or.b32  	%r25, %r23, %r24;
	shr.u32 	%r26, %r3, 2;
	shl.b32 	%r27, %r35, 30;
	or.b32  	%r28, %r27, %r26;
	shr.u32 	%r29, %r36, 30;
	shl.b32 	%r30, %r3, 2;
	or.b32  	%r31, %r29, %r30;
	and.b32  	%r32, %r22, %r28;
	and.b32  	%r33, %r32, %r25;
	and.b32  	%r34, %r33, %r31;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r34;

$L__BB7_6:
	ret;

}
	// .globl	morphoErodeVer_5x5
.entry morphoErodeVer_5x5(
	.param .u64 .ptr .global .align 4 morphoErodeVer_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeVer_5x5_param_1,
	.param .u32 morphoErodeVer_5x5_param_2,
	.param .u32 morphoErodeVer_5x5_param_3,
	.param .u32 morphoErodeVer_5x5_param_4,
	.param .u32 morphoErodeVer_5x5_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd2, [morphoErodeVer_5x5_param_0];
	ld.param.u64 	%rd3, [morphoErodeVer_5x5_param_1];
	ld.param.u32 	%r6, [morphoErodeVer_5x5_param_2];
	ld.param.u32 	%r7, [morphoErodeVer_5x5_param_3];
	ld.param.u32 	%r8, [morphoErodeVer_5x5_param_4];
	ld.param.u32 	%r9, [morphoErodeVer_5x5_param_5];
	mov.b32 	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r14, %r13, %r10;
	mad.lo.s32 	%r1, %r12, %r11, %r14;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %ntid.y;
	mov.u32 	%r17, %tid.y;
	mov.b32 	%r18, %envreg4;
	add.s32 	%r19, %r17, %r18;
	mad.lo.s32 	%r2, %r16, %r15, %r19;
	mul.lo.s32 	%r3, %r2, %r6;
	setp.ge.s32 	%p1, %r2, %r7;
	setp.ge.s32 	%p2, %r1, %r6;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB8_4;

	add.s32 	%r21, %r3, %r1;
	cvt.u64.u32 	%rd1, %r21;
	add.s32 	%r22, %r7, -2;
	setp.le.s32 	%p4, %r22, %r2;
	setp.lt.s32 	%p5, %r2, 2;
	or.pred  	%p6, %p5, %p4;
	mov.u32 	%r41, 0;
	@%p6 bra 	$L__BB8_3;

	shl.b64 	%rd4, %rd1, 2;
	add.s64 	%rd5, %rd2, %rd4;
	ld.global.u32 	%r23, [%rd5];
	shl.b32 	%r24, %r6, 1;
	sub.s32 	%r25, %r3, %r24;
	cvt.s64.s32 	%rd6, %r25;
	cvt.s64.s32 	%rd7, %r1;
	add.s64 	%rd8, %rd6, %rd7;
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd2, %rd9;
	add.s32 	%r26, %r25, %r6;
	cvt.s64.s32 	%rd11, %r26;
	add.s64 	%rd12, %rd11, %rd7;
	shl.b64 	%rd13, %rd12, 2;
	add.s64 	%rd14, %rd2, %rd13;
	add.s32 	%r27, %r26, %r24;
	cvt.s64.s32 	%rd15, %r27;
	add.s64 	%rd16, %rd15, %rd7;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd2, %rd17;
	add.s32 	%r28, %r27, %r6;
	cvt.s64.s32 	%rd19, %r28;
	add.s64 	%rd20, %rd19, %rd7;
	shl.b64 	%rd21, %rd20, 2;
	add.s64 	%rd22, %rd2, %rd21;
	setp.eq.s32 	%p7, %r1, 0;
	selp.b32 	%r29, %r8, -1, %p7;
	add.s32 	%r30, %r6, -1;
	setp.eq.s32 	%p8, %r30, %r1;
	selp.b32 	%r31, %r9, -1, %p8;
	and.b32  	%r32, %r31, %r29;
	and.b32  	%r33, %r32, %r23;
	ld.global.u32 	%r34, [%rd10];
	and.b32  	%r35, %r33, %r34;
	ld.global.u32 	%r36, [%rd14];
	and.b32  	%r37, %r35, %r36;
	ld.global.u32 	%r38, [%rd18];
	and.b32  	%r39, %r37, %r38;
	ld.global.u32 	%r40, [%rd22];
	and.b32  	%r41, %r39, %r40;

$L__BB8_3:
	shl.b64 	%rd23, %rd1, 2;
	add.s64 	%rd24, %rd3, %rd23;
	st.global.u32 	[%rd24], %r41;

$L__BB8_4:
	ret;

}
	// .globl	morphoErodeHor
.entry morphoErodeHor(
	.param .u64 .ptr .global .align 4 morphoErodeHor_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_param_1,
	.param .u32 morphoErodeHor_param_2,
	.param .u32 morphoErodeHor_param_3,
	.param .u32 morphoErodeHor_param_4,
	.param .u32 morphoErodeHor_param_5,
	.param .u8 morphoErodeHor_param_6,
	.param .u32 morphoErodeHor_param_7,
	.param .u32 morphoErodeHor_param_8
)
{
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<566>;
	.reg .b64 	%rd<37>;


	ld.param.s8 	%rs1, [morphoErodeHor_param_6];
	ld.param.u64 	%rd12, [morphoErodeHor_param_0];
	ld.param.u64 	%rd13, [morphoErodeHor_param_1];
	ld.param.u32 	%r91, [morphoErodeHor_param_2];
	ld.param.u32 	%r92, [morphoErodeHor_param_3];
	ld.param.u32 	%r93, [morphoErodeHor_param_4];
	ld.param.u32 	%r96, [morphoErodeHor_param_5];
	ld.param.u32 	%r94, [morphoErodeHor_param_7];
	ld.param.u32 	%r95, [morphoErodeHor_param_8];
	mov.b32 	%r97, %envreg3;
	mov.u32 	%r98, %ctaid.x;
	mov.u32 	%r99, %ntid.x;
	mul.lo.s32 	%r1, %r99, %r98;
	mov.u32 	%r100, %tid.x;
	add.s32 	%r2, %r100, %r97;
	add.s32 	%r3, %r2, %r1;
	mov.u32 	%r101, %ctaid.y;
	mov.u32 	%r102, %ntid.y;
	mov.u32 	%r103, %tid.y;
	mov.b32 	%r104, %envreg4;
	add.s32 	%r105, %r103, %r104;
	mad.lo.s32 	%r106, %r102, %r101, %r105;
	mul.lo.s32 	%r4, %r106, %r93;
	add.s32 	%r5, %r4, %r3;
	mul.lo.s32 	%r107, %r96, %r93;
	setp.ge.u32 	%p1, %r5, %r107;
	@%p1 bra 	$L__BB9_43;

	setp.lt.s32 	%p2, %r92, 1;
	setp.lt.s32 	%p3, %r91, 1;
	and.pred  	%p4, %p3, %p2;
	@%p4 bra 	$L__BB9_43;

	cvt.u64.u32 	%rd1, %r5;
	mul.wide.u32 	%rd14, %r5, 4;
	add.s64 	%rd2, %rd12, %rd14;
	ld.global.u32 	%r6, [%rd2];
	and.b32  	%r7, %r92, 31;
	and.b32  	%r109, %r91, 31;
	setp.ne.s32 	%p5, %r109, 0;
	selp.b32 	%r8, %r109, 31, %p5;
	selp.u32 	%r9, 1, 0, %p5;
	shr.s32 	%r10, %r91, 5;
	add.s32 	%r11, %r10, %r9;
	sub.s32 	%r12, %r3, %r11;
	add.s32 	%r13, %r11, %r3;
	setp.eq.s32 	%p6, %r3, 0;
	mov.u32 	%r530, -1;
	mov.u32 	%r529, %r530;
	@%p6 bra 	$L__BB9_4;

	ld.global.u32 	%r529, [%rd2+-4];

$L__BB9_4:
	add.s32 	%r16, %r93, -1;
	setp.eq.s32 	%p7, %r16, %r3;
	@%p7 bra 	$L__BB9_6;

	ld.global.u32 	%r530, [%rd2+4];

$L__BB9_6:
	add.s32 	%r19, %r8, -1;
	and.b32  	%r540, %r8, 3;
	setp.eq.s32 	%p8, %r8, %r7;
	@%p8 bra 	$L__BB9_13;
	bra.uni 	$L__BB9_7;

$L__BB9_13:
	setp.lt.u32 	%p18, %r19, 3;
	mov.u32 	%r545, 1;
	mov.u32 	%r551, %r6;
	@%p18 bra 	$L__BB9_16;

	sub.s32 	%r543, %r7, %r540;
	mov.u32 	%r545, 1;
	mov.u32 	%r551, %r6;

$L__BB9_15:
	neg.s32 	%r179, %r545;
	and.b32  	%r180, %r179, 31;
	shl.b32 	%r181, %r529, %r180;
	and.b32  	%r182, %r545, 31;
	shr.u32 	%r183, %r6, %r182;
	or.b32  	%r184, %r181, %r183;
	and.b32  	%r185, %r184, %r551;
	shl.b32 	%r186, %r6, %r182;
	shr.u32 	%r187, %r530, %r180;
	or.b32  	%r188, %r186, %r187;
	and.b32  	%r189, %r185, %r188;
	xor.b32  	%r190, %r182, 31;
	shl.b32 	%r191, %r529, %r190;
	add.s32 	%r192, %r545, 1;
	and.b32  	%r193, %r192, 31;
	shr.u32 	%r194, %r6, %r193;
	or.b32  	%r195, %r191, %r194;
	and.b32  	%r196, %r195, %r189;
	shl.b32 	%r197, %r6, %r193;
	shr.u32 	%r198, %r530, %r190;
	or.b32  	%r199, %r197, %r198;
	and.b32  	%r200, %r196, %r199;
	mov.u32 	%r201, 30;
	sub.s32 	%r202, %r201, %r545;
	and.b32  	%r203, %r202, 31;
	shl.b32 	%r204, %r529, %r203;
	add.s32 	%r205, %r545, 2;
	and.b32  	%r206, %r205, 31;
	shr.u32 	%r207, %r6, %r206;
	or.b32  	%r208, %r204, %r207;
	and.b32  	%r209, %r208, %r200;
	shl.b32 	%r210, %r6, %r206;
	shr.u32 	%r211, %r530, %r203;
	or.b32  	%r212, %r210, %r211;
	and.b32  	%r213, %r209, %r212;
	mov.u32 	%r214, 29;
	sub.s32 	%r215, %r214, %r545;
	and.b32  	%r216, %r215, 31;
	shl.b32 	%r217, %r529, %r216;
	add.s32 	%r218, %r545, 3;
	and.b32  	%r219, %r218, 31;
	shr.u32 	%r220, %r6, %r219;
	or.b32  	%r221, %r217, %r220;
	and.b32  	%r222, %r221, %r213;
	shl.b32 	%r223, %r6, %r219;
	shr.u32 	%r224, %r530, %r216;
	or.b32  	%r225, %r223, %r224;
	and.b32  	%r551, %r222, %r225;
	add.s32 	%r545, %r545, 4;
	add.s32 	%r543, %r543, -4;
	setp.ne.s32 	%p19, %r543, 0;
	@%p19 bra 	$L__BB9_15;

$L__BB9_16:
	setp.eq.s32 	%p20, %r540, 0;
	@%p20 bra 	$L__BB9_19;

	neg.s32 	%r547, %r545;

$L__BB9_18:
	.pragma "nounroll";
	and.b32  	%r226, %r547, 31;
	shl.b32 	%r227, %r529, %r226;
	and.b32  	%r228, %r545, 31;
	shr.u32 	%r229, %r6, %r228;
	or.b32  	%r230, %r227, %r229;
	and.b32  	%r231, %r230, %r551;
	shl.b32 	%r232, %r6, %r228;
	shr.u32 	%r233, %r530, %r226;
	or.b32  	%r234, %r232, %r233;
	and.b32  	%r551, %r231, %r234;
	add.s32 	%r545, %r545, 1;
	add.s32 	%r547, %r547, -1;
	add.s32 	%r540, %r540, -1;
	setp.ne.s32 	%p21, %r540, 0;
	@%p21 bra 	$L__BB9_18;
	bra.uni 	$L__BB9_19;

$L__BB9_7:
	setp.lt.u32 	%p9, %r19, 3;
	mov.u32 	%r535, 1;
	mov.u32 	%r551, %r6;
	@%p9 bra 	$L__BB9_10;

	sub.s32 	%r533, %r8, %r540;
	mov.u32 	%r535, 1;
	mov.u32 	%r551, %r6;

$L__BB9_9:
	neg.s32 	%r114, %r535;
	and.b32  	%r115, %r114, 31;
	shl.b32 	%r116, %r529, %r115;
	and.b32  	%r117, %r535, 31;
	shr.u32 	%r118, %r6, %r117;
	or.b32  	%r119, %r116, %r118;
	and.b32  	%r120, %r119, %r551;
	shl.b32 	%r121, %r6, %r117;
	shr.u32 	%r122, %r530, %r115;
	or.b32  	%r123, %r121, %r122;
	setp.eq.s32 	%p10, %r535, %r8;
	selp.b32 	%r124, -1, %r123, %p10;
	and.b32  	%r125, %r120, %r124;
	xor.b32  	%r126, %r117, 31;
	shl.b32 	%r127, %r529, %r126;
	add.s32 	%r128, %r535, 1;
	and.b32  	%r129, %r128, 31;
	shr.u32 	%r130, %r6, %r129;
	or.b32  	%r131, %r127, %r130;
	and.b32  	%r132, %r131, %r125;
	setp.eq.s32 	%p11, %r128, %r8;
	shl.b32 	%r133, %r6, %r129;
	shr.u32 	%r134, %r530, %r126;
	or.b32  	%r135, %r133, %r134;
	selp.b32 	%r136, -1, %r135, %p11;
	and.b32  	%r137, %r132, %r136;
	mov.u32 	%r138, 30;
	sub.s32 	%r139, %r138, %r535;
	and.b32  	%r140, %r139, 31;
	shl.b32 	%r141, %r529, %r140;
	add.s32 	%r142, %r535, 2;
	and.b32  	%r143, %r142, 31;
	shr.u32 	%r144, %r6, %r143;
	or.b32  	%r145, %r141, %r144;
	and.b32  	%r146, %r145, %r137;
	setp.eq.s32 	%p12, %r142, %r8;
	shl.b32 	%r147, %r6, %r143;
	shr.u32 	%r148, %r530, %r140;
	or.b32  	%r149, %r147, %r148;
	selp.b32 	%r150, -1, %r149, %p12;
	and.b32  	%r151, %r146, %r150;
	mov.u32 	%r152, 29;
	sub.s32 	%r153, %r152, %r535;
	and.b32  	%r154, %r153, 31;
	shl.b32 	%r155, %r529, %r154;
	add.s32 	%r156, %r535, 3;
	and.b32  	%r157, %r156, 31;
	shr.u32 	%r158, %r6, %r157;
	or.b32  	%r159, %r155, %r158;
	and.b32  	%r160, %r159, %r151;
	setp.eq.s32 	%p13, %r156, %r8;
	shl.b32 	%r161, %r6, %r157;
	shr.u32 	%r162, %r530, %r154;
	or.b32  	%r163, %r161, %r162;
	selp.b32 	%r164, -1, %r163, %p13;
	and.b32  	%r551, %r160, %r164;
	add.s32 	%r535, %r535, 4;
	add.s32 	%r533, %r533, -4;
	setp.ne.s32 	%p14, %r533, 0;
	@%p14 bra 	$L__BB9_9;

$L__BB9_10:
	setp.eq.s32 	%p15, %r540, 0;
	@%p15 bra 	$L__BB9_19;

	sub.s32 	%r537, %r535, %r8;

$L__BB9_12:
	.pragma "nounroll";
	neg.s32 	%r165, %r535;
	and.b32  	%r166, %r165, 31;
	shl.b32 	%r167, %r529, %r166;
	and.b32  	%r168, %r535, 31;
	shr.u32 	%r169, %r6, %r168;
	or.b32  	%r170, %r167, %r169;
	and.b32  	%r171, %r170, %r551;
	shl.b32 	%r172, %r6, %r168;
	shr.u32 	%r173, %r530, %r166;
	or.b32  	%r174, %r172, %r173;
	setp.eq.s32 	%p16, %r537, 0;
	selp.b32 	%r175, -1, %r174, %p16;
	and.b32  	%r551, %r171, %r175;
	add.s32 	%r535, %r535, 1;
	add.s32 	%r537, %r537, 1;
	add.s32 	%r540, %r540, -1;
	setp.eq.s32 	%p17, %r540, 0;
	@%p17 bra 	$L__BB9_19;
	bra.uni 	$L__BB9_12;

$L__BB9_19:
	shl.b64 	%rd15, %rd1, 2;
	add.s64 	%rd3, %rd13, %rd15;
	setp.eq.s32 	%p22, %r11, 1;
	@%p22 bra 	$L__BB9_42;
	bra.uni 	$L__BB9_20;

$L__BB9_42:
	setp.eq.s32 	%p37, %r91, 32;
	selp.b32 	%r520, %r529, -1, %p37;
	setp.eq.s32 	%p38, %r92, 32;
	selp.b32 	%r521, %r530, -1, %p38;
	and.b32  	%r522, %r521, %r520;
	and.b32  	%r523, %r522, %r551;
	selp.b32 	%r524, %r94, -1, %p6;
	selp.b32 	%r525, %r95, -1, %p7;
	and.b32  	%r526, %r525, %r524;
	setp.eq.s16 	%p40, %rs1, 0;
	selp.b32 	%r527, -1, %r526, %p40;
	and.b32  	%r528, %r523, %r527;
	st.global.u32 	[%rd3], %r528;
	bra.uni 	$L__BB9_43;

$L__BB9_20:
	setp.lt.s32 	%p23, %r12, 0;
	mov.u32 	%r558, -1;
	mov.u32 	%r557, %r558;
	@%p23 bra 	$L__BB9_22;

	cvt.s64.s32 	%rd16, %r4;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd16;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd12, %rd19;
	ld.global.u32 	%r557, [%rd20];

$L__BB9_22:
	setp.ge.s32 	%p24, %r13, %r93;
	@%p24 bra 	$L__BB9_24;

	cvt.s64.s32 	%rd21, %r4;
	cvt.s64.s32 	%rd22, %r13;
	add.s64 	%rd23, %rd22, %rd21;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd25, %rd12, %rd24;
	ld.global.u32 	%r558, [%rd25];

$L__BB9_24:
	setp.lt.u32 	%p25, %r11, 2;
	@%p25 bra 	$L__BB9_36;

	neg.s32 	%r238, %r8;
	and.b32  	%r64, %r238, 31;
	neg.s32 	%r239, %r92;
	and.b32  	%r65, %r239, 31;
	add.s32 	%r240, %r2, %r10;
	add.s32 	%r241, %r240, %r1;
	add.s32 	%r242, %r241, %r9;
	add.s32 	%r555, %r242, -1;
	add.s32 	%r243, %r3, 1;
	mov.u32 	%r556, 1;
	sub.s32 	%r244, %r243, %r9;
	sub.s32 	%r554, %r244, %r10;
	cvt.s64.s32 	%rd26, %r13;
	cvt.s64.s32 	%rd27, %r4;
	add.s64 	%rd28, %rd27, %rd26;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd12, %rd29;
	add.s64 	%rd36, %rd30, -4;
	cvt.s64.s32 	%rd31, %r12;
	add.s64 	%rd32, %rd27, %rd31;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd34, %rd12, %rd33;
	add.s64 	%rd35, %rd34, 8;

$L__BB9_26:
	add.s64 	%rd8, %rd35, -4;
	setp.lt.s32 	%p26, %r554, 0;
	mov.u32 	%r561, -1;
	mov.u32 	%r560, %r561;
	@%p26 bra 	$L__BB9_28;

	ld.global.u32 	%r560, [%rd8];

$L__BB9_28:
	setp.lt.s32 	%p27, %r554, -1;
	@%p27 bra 	$L__BB9_30;

	ld.global.u32 	%r561, [%rd8+4];

$L__BB9_30:
	add.s64 	%rd36, %rd36, -4;
	setp.ge.s32 	%p28, %r555, %r93;
	mov.u32 	%r562, -1;
	@%p28 bra 	$L__BB9_32;

	ld.global.u32 	%r562, [%rd36+4];

$L__BB9_32:
	add.s32 	%r555, %r555, -1;
	setp.lt.s32 	%p29, %r555, %r93;
	@%p29 bra 	$L__BB9_34;
	bra.uni 	$L__BB9_33;

$L__BB9_34:
	ld.global.u32 	%r563, [%rd36];
	bra.uni 	$L__BB9_35;

$L__BB9_33:
	mov.u32 	%r563, -1;

$L__BB9_35:
	shl.b32 	%r249, %r557, %r64;
	shr.u32 	%r250, %r560, %r8;
	or.b32  	%r251, %r250, %r249;
	shr.u32 	%r252, %r561, %r8;
	shl.b32 	%r253, %r560, %r64;
	or.b32  	%r254, %r252, %r253;
	shl.b32 	%r255, %r562, %r7;
	shr.u32 	%r256, %r558, %r65;
	or.b32  	%r257, %r255, %r256;
	shr.u32 	%r258, %r562, %r65;
	shl.b32 	%r259, %r563, %r7;
	or.b32  	%r260, %r259, %r258;
	or.b32  	%r261, %r251, %r254;
	and.b32  	%r262, %r261, %r551;
	or.b32  	%r263, %r260, %r257;
	and.b32  	%r264, %r262, %r263;
	shr.u32 	%r265, %r254, 31;
	shl.b32 	%r266, %r251, 1;
	or.b32  	%r267, %r266, %r265;
	and.b32  	%r268, %r267, %r264;
	shl.b32 	%r269, %r260, 1;
	shr.u32 	%r270, %r257, 31;
	or.b32  	%r271, %r269, %r270;
	and.b32  	%r272, %r268, %r271;
	shr.u32 	%r273, %r254, 30;
	shl.b32 	%r274, %r251, 2;
	or.b32  	%r275, %r274, %r273;
	and.b32  	%r276, %r275, %r272;
	shl.b32 	%r277, %r260, 2;
	shr.u32 	%r278, %r257, 30;
	or.b32  	%r279, %r277, %r278;
	and.b32  	%r280, %r276, %r279;
	shr.u32 	%r281, %r254, 29;
	shl.b32 	%r282, %r251, 3;
	or.b32  	%r283, %r282, %r281;
	and.b32  	%r284, %r283, %r280;
	shl.b32 	%r285, %r260, 3;
	shr.u32 	%r286, %r257, 29;
	or.b32  	%r287, %r285, %r286;
	and.b32  	%r288, %r284, %r287;
	shr.u32 	%r289, %r254, 28;
	shl.b32 	%r290, %r251, 4;
	or.b32  	%r291, %r290, %r289;
	and.b32  	%r292, %r291, %r288;
	shl.b32 	%r293, %r260, 4;
	shr.u32 	%r294, %r257, 28;
	or.b32  	%r295, %r293, %r294;
	and.b32  	%r296, %r292, %r295;
	shr.u32 	%r297, %r254, 27;
	shl.b32 	%r298, %r251, 5;
	or.b32  	%r299, %r298, %r297;
	and.b32  	%r300, %r299, %r296;
	shl.b32 	%r301, %r260, 5;
	shr.u32 	%r302, %r257, 27;
	or.b32  	%r303, %r301, %r302;
	and.b32  	%r304, %r300, %r303;
	shr.u32 	%r305, %r254, 26;
	shl.b32 	%r306, %r251, 6;
	or.b32  	%r307, %r306, %r305;
	and.b32  	%r308, %r307, %r304;
	shl.b32 	%r309, %r260, 6;
	shr.u32 	%r310, %r257, 26;
	or.b32  	%r311, %r309, %r310;
	and.b32  	%r312, %r308, %r311;
	shr.u32 	%r313, %r254, 25;
	shl.b32 	%r314, %r251, 7;
	or.b32  	%r315, %r314, %r313;
	and.b32  	%r316, %r315, %r312;
	shl.b32 	%r317, %r260, 7;
	shr.u32 	%r318, %r257, 25;
	or.b32  	%r319, %r317, %r318;
	and.b32  	%r320, %r316, %r319;
	shr.u32 	%r321, %r254, 24;
	shl.b32 	%r322, %r251, 8;
	or.b32  	%r323, %r322, %r321;
	and.b32  	%r324, %r323, %r320;
	shl.b32 	%r325, %r260, 8;
	shr.u32 	%r326, %r257, 24;
	or.b32  	%r327, %r325, %r326;
	and.b32  	%r328, %r324, %r327;
	shr.u32 	%r329, %r254, 23;
	shl.b32 	%r330, %r251, 9;
	or.b32  	%r331, %r330, %r329;
	and.b32  	%r332, %r331, %r328;
	shl.b32 	%r333, %r260, 9;
	shr.u32 	%r334, %r257, 23;
	or.b32  	%r335, %r333, %r334;
	and.b32  	%r336, %r332, %r335;
	shr.u32 	%r337, %r254, 22;
	shl.b32 	%r338, %r251, 10;
	or.b32  	%r339, %r338, %r337;
	and.b32  	%r340, %r339, %r336;
	shl.b32 	%r341, %r260, 10;
	shr.u32 	%r342, %r257, 22;
	or.b32  	%r343, %r341, %r342;
	and.b32  	%r344, %r340, %r343;
	shr.u32 	%r345, %r254, 21;
	shl.b32 	%r346, %r251, 11;
	or.b32  	%r347, %r346, %r345;
	and.b32  	%r348, %r347, %r344;
	shl.b32 	%r349, %r260, 11;
	shr.u32 	%r350, %r257, 21;
	or.b32  	%r351, %r349, %r350;
	and.b32  	%r352, %r348, %r351;
	shr.u32 	%r353, %r254, 20;
	shl.b32 	%r354, %r251, 12;
	or.b32  	%r355, %r354, %r353;
	and.b32  	%r356, %r355, %r352;
	shl.b32 	%r357, %r260, 12;
	shr.u32 	%r358, %r257, 20;
	or.b32  	%r359, %r357, %r358;
	and.b32  	%r360, %r356, %r359;
	shr.u32 	%r361, %r254, 19;
	shl.b32 	%r362, %r251, 13;
	or.b32  	%r363, %r362, %r361;
	and.b32  	%r364, %r363, %r360;
	shl.b32 	%r365, %r260, 13;
	shr.u32 	%r366, %r257, 19;
	or.b32  	%r367, %r365, %r366;
	and.b32  	%r368, %r364, %r367;
	shr.u32 	%r369, %r254, 18;
	shl.b32 	%r370, %r251, 14;
	or.b32  	%r371, %r370, %r369;
	and.b32  	%r372, %r371, %r368;
	shl.b32 	%r373, %r260, 14;
	shr.u32 	%r374, %r257, 18;
	or.b32  	%r375, %r373, %r374;
	and.b32  	%r376, %r372, %r375;
	shr.u32 	%r377, %r254, 17;
	shl.b32 	%r378, %r251, 15;
	or.b32  	%r379, %r378, %r377;
	and.b32  	%r380, %r379, %r376;
	shl.b32 	%r381, %r260, 15;
	shr.u32 	%r382, %r257, 17;
	or.b32  	%r383, %r381, %r382;
	and.b32  	%r384, %r380, %r383;
	shr.u32 	%r385, %r254, 16;
	shl.b32 	%r386, %r251, 16;
	or.b32  	%r387, %r386, %r385;
	and.b32  	%r388, %r387, %r384;
	shl.b32 	%r389, %r260, 16;
	shr.u32 	%r390, %r257, 16;
	or.b32  	%r391, %r389, %r390;
	and.b32  	%r392, %r388, %r391;
	shr.u32 	%r393, %r254, 15;
	shl.b32 	%r394, %r251, 17;
	or.b32  	%r395, %r394, %r393;
	and.b32  	%r396, %r395, %r392;
	shl.b32 	%r397, %r260, 17;
	shr.u32 	%r398, %r257, 15;
	or.b32  	%r399, %r397, %r398;
	and.b32  	%r400, %r396, %r399;
	shr.u32 	%r401, %r254, 14;
	shl.b32 	%r402, %r251, 18;
	or.b32  	%r403, %r402, %r401;
	and.b32  	%r404, %r403, %r400;
	shl.b32 	%r405, %r260, 18;
	shr.u32 	%r406, %r257, 14;
	or.b32  	%r407, %r405, %r406;
	and.b32  	%r408, %r404, %r407;
	shr.u32 	%r409, %r254, 13;
	shl.b32 	%r410, %r251, 19;
	or.b32  	%r411, %r410, %r409;
	and.b32  	%r412, %r411, %r408;
	shl.b32 	%r413, %r260, 19;
	shr.u32 	%r414, %r257, 13;
	or.b32  	%r415, %r413, %r414;
	and.b32  	%r416, %r412, %r415;
	shr.u32 	%r417, %r254, 12;
	shl.b32 	%r418, %r251, 20;
	or.b32  	%r419, %r418, %r417;
	and.b32  	%r420, %r419, %r416;
	shl.b32 	%r421, %r260, 20;
	shr.u32 	%r422, %r257, 12;
	or.b32  	%r423, %r421, %r422;
	and.b32  	%r424, %r420, %r423;
	shr.u32 	%r425, %r254, 11;
	shl.b32 	%r426, %r251, 21;
	or.b32  	%r427, %r426, %r425;
	and.b32  	%r428, %r427, %r424;
	shl.b32 	%r429, %r260, 21;
	shr.u32 	%r430, %r257, 11;
	or.b32  	%r431, %r429, %r430;
	and.b32  	%r432, %r428, %r431;
	shr.u32 	%r433, %r254, 10;
	shl.b32 	%r434, %r251, 22;
	or.b32  	%r435, %r434, %r433;
	and.b32  	%r436, %r435, %r432;
	shl.b32 	%r437, %r260, 22;
	shr.u32 	%r438, %r257, 10;
	or.b32  	%r439, %r437, %r438;
	and.b32  	%r440, %r436, %r439;
	shr.u32 	%r441, %r254, 9;
	shl.b32 	%r442, %r251, 23;
	or.b32  	%r443, %r442, %r441;
	and.b32  	%r444, %r443, %r440;
	shl.b32 	%r445, %r260, 23;
	shr.u32 	%r446, %r257, 9;
	or.b32  	%r447, %r445, %r446;
	and.b32  	%r448, %r444, %r447;
	shr.u32 	%r449, %r254, 8;
	shl.b32 	%r450, %r251, 24;
	or.b32  	%r451, %r450, %r449;
	and.b32  	%r452, %r451, %r448;
	shl.b32 	%r453, %r260, 24;
	shr.u32 	%r454, %r257, 8;
	or.b32  	%r455, %r453, %r454;
	and.b32  	%r456, %r452, %r455;
	shr.u32 	%r457, %r254, 7;
	shl.b32 	%r458, %r251, 25;
	or.b32  	%r459, %r458, %r457;
	and.b32  	%r460, %r459, %r456;
	shl.b32 	%r461, %r260, 25;
	shr.u32 	%r462, %r257, 7;
	or.b32  	%r463, %r461, %r462;
	and.b32  	%r464, %r460, %r463;
	shr.u32 	%r465, %r254, 6;
	shl.b32 	%r466, %r251, 26;
	or.b32  	%r467, %r466, %r465;
	and.b32  	%r468, %r467, %r464;
	shl.b32 	%r469, %r260, 26;
	shr.u32 	%r470, %r257, 6;
	or.b32  	%r471, %r469, %r470;
	and.b32  	%r472, %r468, %r471;
	shr.u32 	%r473, %r254, 5;
	shl.b32 	%r474, %r251, 27;
	or.b32  	%r475, %r474, %r473;
	and.b32  	%r476, %r475, %r472;
	shl.b32 	%r477, %r260, 27;
	shr.u32 	%r478, %r257, 5;
	or.b32  	%r479, %r477, %r478;
	and.b32  	%r480, %r476, %r479;
	shr.u32 	%r481, %r254, 4;
	shl.b32 	%r482, %r251, 28;
	or.b32  	%r483, %r482, %r481;
	and.b32  	%r484, %r483, %r480;
	shl.b32 	%r485, %r260, 28;
	shr.u32 	%r486, %r257, 4;
	or.b32  	%r487, %r485, %r486;
	and.b32  	%r488, %r484, %r487;
	shr.u32 	%r489, %r254, 3;
	shl.b32 	%r490, %r251, 29;
	or.b32  	%r491, %r490, %r489;
	and.b32  	%r492, %r491, %r488;
	shl.b32 	%r493, %r260, 29;
	shr.u32 	%r494, %r257, 3;
	or.b32  	%r495, %r493, %r494;
	and.b32  	%r496, %r492, %r495;
	shr.u32 	%r497, %r254, 2;
	shl.b32 	%r498, %r251, 30;
	or.b32  	%r499, %r498, %r497;
	and.b32  	%r500, %r499, %r496;
	shl.b32 	%r501, %r260, 30;
	shr.u32 	%r502, %r257, 2;
	or.b32  	%r503, %r501, %r502;
	and.b32  	%r504, %r500, %r503;
	shr.u32 	%r505, %r254, 1;
	shl.b32 	%r506, %r251, 31;
	or.b32  	%r507, %r506, %r505;
	and.b32  	%r508, %r507, %r504;
	shl.b32 	%r509, %r260, 31;
	shr.u32 	%r510, %r257, 1;
	or.b32  	%r511, %r509, %r510;
	and.b32  	%r512, %r508, %r511;
	and.b32  	%r513, %r254, %r251;
	and.b32  	%r514, %r513, %r257;
	and.b32  	%r515, %r514, %r260;
	and.b32  	%r551, %r515, %r512;
	add.s32 	%r554, %r554, 1;
	add.s64 	%rd35, %rd35, 4;
	add.s32 	%r556, %r556, 1;
	setp.lt.u32 	%p30, %r556, %r11;
	mov.u32 	%r557, %r561;
	mov.u32 	%r558, %r563;
	@%p30 bra 	$L__BB9_26;

$L__BB9_36:
	setp.eq.s16 	%p31, %rs1, 0;
	mov.u32 	%r565, %r551;
	@%p31 bra 	$L__BB9_41;

	add.s32 	%r87, %r11, -1;
	setp.gt.u32 	%p32, %r87, %r3;
	mov.u32 	%r565, 0;
	@%p32 bra 	$L__BB9_41;

	setp.eq.s32 	%p33, %r87, %r3;
	@%p33 bra 	$L__BB9_40;
	bra.uni 	$L__BB9_39;

$L__BB9_40:
	and.b32  	%r565, %r551, %r94;
	bra.uni 	$L__BB9_41;

$L__BB9_39:
	sub.s32 	%r517, %r93, %r11;
	setp.lt.u32 	%p34, %r517, %r3;
	setp.eq.s32 	%p35, %r517, %r3;
	selp.b32 	%r518, %r95, -1, %p35;
	and.b32  	%r519, %r551, %r518;
	selp.b32 	%r565, 0, %r519, %p34;

$L__BB9_41:
	st.global.u32 	[%rd3], %r565;

$L__BB9_43:
	ret;

}
	// .globl	morphoErodeHor_32word
.entry morphoErodeHor_32word(
	.param .u64 .ptr .global .align 4 morphoErodeHor_32word_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_32word_param_1,
	.param .u32 morphoErodeHor_32word_param_2,
	.param .u32 morphoErodeHor_32word_param_3,
	.param .u32 morphoErodeHor_32word_param_4,
	.param .u8 morphoErodeHor_32word_param_5,
	.param .u32 morphoErodeHor_32word_param_6,
	.param .u32 morphoErodeHor_32word_param_7,
	.param .u8 morphoErodeHor_32word_param_8
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<220>;
	.reg .b64 	%rd<8>;


	ld.param.s8 	%rs2, [morphoErodeHor_32word_param_8];
	ld.param.s8 	%rs1, [morphoErodeHor_32word_param_5];
	ld.param.u64 	%rd3, [morphoErodeHor_32word_param_0];
	ld.param.u64 	%rd4, [morphoErodeHor_32word_param_1];
	ld.param.u32 	%r50, [morphoErodeHor_32word_param_2];
	ld.param.u32 	%r51, [morphoErodeHor_32word_param_3];
	ld.param.u32 	%r54, [morphoErodeHor_32word_param_4];
	ld.param.u32 	%r52, [morphoErodeHor_32word_param_6];
	ld.param.u32 	%r53, [morphoErodeHor_32word_param_7];
	mov.b32 	%r55, %envreg3;
	mov.u32 	%r56, %ctaid.x;
	mov.u32 	%r57, %ntid.x;
	mov.u32 	%r58, %tid.x;
	add.s32 	%r59, %r58, %r55;
	mad.lo.s32 	%r1, %r57, %r56, %r59;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r61, %ntid.y;
	mov.u32 	%r62, %tid.y;
	mov.b32 	%r63, %envreg4;
	add.s32 	%r64, %r62, %r63;
	mad.lo.s32 	%r65, %r61, %r60, %r64;
	mad.lo.s32 	%r2, %r65, %r51, %r1;
	mul.lo.s32 	%r66, %r54, %r51;
	setp.ge.u32 	%p1, %r2, %r66;
	@%p1 bra 	$L__BB10_20;

	cvt.u64.u32 	%rd1, %r2;
	mul.wide.u32 	%rd5, %r2, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32 	%p2, %r1, 0;
	mov.u32 	%r198, -1;
	mov.u32 	%r197, %r198;
	@%p2 bra 	$L__BB10_3;

	ld.global.u32 	%r197, [%rd2+-4];

$L__BB10_3:
	add.s32 	%r6, %r51, -1;
	setp.eq.s32 	%p3, %r6, %r1;
	@%p3 bra 	$L__BB10_5;

	ld.global.u32 	%r198, [%rd2+4];

$L__BB10_5:
	setp.lt.s32 	%p4, %r50, 1;
	mov.u32 	%r219, %r3;
	@%p4 bra 	$L__BB10_19;

	setp.eq.s16 	%p5, %rs2, 0;
	add.s32 	%r9, %r50, -1;
	and.b32  	%r208, %r50, 3;
	@%p5 bra 	$L__BB10_13;

	setp.lt.u32 	%p6, %r9, 3;
	mov.u32 	%r203, 1;
	mov.u32 	%r219, %r3;
	@%p6 bra 	$L__BB10_10;

	sub.s32 	%r201, %r50, %r208;
	mov.u32 	%r203, 1;
	mov.u32 	%r219, %r3;

$L__BB10_9:
	neg.s32 	%r72, %r203;
	and.b32  	%r73, %r72, 31;
	shl.b32 	%r74, %r197, %r73;
	and.b32  	%r75, %r203, 31;
	shr.u32 	%r76, %r3, %r75;
	or.b32  	%r77, %r74, %r76;
	and.b32  	%r78, %r77, %r219;
	shl.b32 	%r79, %r3, %r75;
	shr.u32 	%r80, %r198, %r73;
	or.b32  	%r81, %r79, %r80;
	setp.eq.s32 	%p7, %r203, %r50;
	selp.b32 	%r82, -1, %r81, %p7;
	and.b32  	%r83, %r78, %r82;
	xor.b32  	%r84, %r75, 31;
	shl.b32 	%r85, %r197, %r84;
	add.s32 	%r86, %r203, 1;
	and.b32  	%r87, %r86, 31;
	shr.u32 	%r88, %r3, %r87;
	or.b32  	%r89, %r85, %r88;
	and.b32  	%r90, %r89, %r83;
	setp.eq.s32 	%p8, %r86, %r50;
	shl.b32 	%r91, %r3, %r87;
	shr.u32 	%r92, %r198, %r84;
	or.b32  	%r93, %r91, %r92;
	selp.b32 	%r94, -1, %r93, %p8;
	and.b32  	%r95, %r90, %r94;
	mov.u32 	%r96, 30;
	sub.s32 	%r97, %r96, %r203;
	and.b32  	%r98, %r97, 31;
	shl.b32 	%r99, %r197, %r98;
	add.s32 	%r100, %r203, 2;
	and.b32  	%r101, %r100, 31;
	shr.u32 	%r102, %r3, %r101;
	or.b32  	%r103, %r99, %r102;
	and.b32  	%r104, %r103, %r95;
	setp.eq.s32 	%p9, %r100, %r50;
	shl.b32 	%r105, %r3, %r101;
	shr.u32 	%r106, %r198, %r98;
	or.b32  	%r107, %r105, %r106;
	selp.b32 	%r108, -1, %r107, %p9;
	and.b32  	%r109, %r104, %r108;
	mov.u32 	%r110, 29;
	sub.s32 	%r111, %r110, %r203;
	and.b32  	%r112, %r111, 31;
	shl.b32 	%r113, %r197, %r112;
	add.s32 	%r114, %r203, 3;
	and.b32  	%r115, %r114, 31;
	shr.u32 	%r116, %r3, %r115;
	or.b32  	%r117, %r113, %r116;
	and.b32  	%r118, %r117, %r109;
	setp.eq.s32 	%p10, %r114, %r50;
	shl.b32 	%r119, %r3, %r115;
	shr.u32 	%r120, %r198, %r112;
	or.b32  	%r121, %r119, %r120;
	selp.b32 	%r122, -1, %r121, %p10;
	and.b32  	%r219, %r118, %r122;
	add.s32 	%r203, %r203, 4;
	add.s32 	%r201, %r201, -4;
	setp.ne.s32 	%p11, %r201, 0;
	@%p11 bra 	$L__BB10_9;

$L__BB10_10:
	setp.eq.s32 	%p12, %r208, 0;
	@%p12 bra 	$L__BB10_19;

	sub.s32 	%r205, %r203, %r50;

$L__BB10_12:
	.pragma "nounroll";
	neg.s32 	%r123, %r203;
	and.b32  	%r124, %r123, 31;
	shl.b32 	%r125, %r197, %r124;
	and.b32  	%r126, %r203, 31;
	shr.u32 	%r127, %r3, %r126;
	or.b32  	%r128, %r125, %r127;
	and.b32  	%r129, %r128, %r219;
	shl.b32 	%r130, %r3, %r126;
	shr.u32 	%r131, %r198, %r124;
	or.b32  	%r132, %r130, %r131;
	setp.eq.s32 	%p13, %r205, 0;
	selp.b32 	%r133, -1, %r132, %p13;
	and.b32  	%r219, %r129, %r133;
	add.s32 	%r203, %r203, 1;
	add.s32 	%r205, %r205, 1;
	add.s32 	%r208, %r208, -1;
	setp.eq.s32 	%p14, %r208, 0;
	@%p14 bra 	$L__BB10_19;
	bra.uni 	$L__BB10_12;

$L__BB10_13:
	setp.lt.u32 	%p15, %r9, 3;
	mov.u32 	%r213, 1;
	mov.u32 	%r219, %r3;
	@%p15 bra 	$L__BB10_16;

	sub.s32 	%r211, %r50, %r208;
	mov.u32 	%r213, 1;
	mov.u32 	%r219, %r3;

$L__BB10_15:
	neg.s32 	%r137, %r213;
	and.b32  	%r138, %r137, 31;
	shl.b32 	%r139, %r197, %r138;
	and.b32  	%r140, %r213, 31;
	shr.u32 	%r141, %r3, %r140;
	or.b32  	%r142, %r139, %r141;
	and.b32  	%r143, %r142, %r219;
	shl.b32 	%r144, %r3, %r140;
	shr.u32 	%r145, %r198, %r138;
	or.b32  	%r146, %r144, %r145;
	and.b32  	%r147, %r143, %r146;
	xor.b32  	%r148, %r140, 31;
	shl.b32 	%r149, %r197, %r148;
	add.s32 	%r150, %r213, 1;
	and.b32  	%r151, %r150, 31;
	shr.u32 	%r152, %r3, %r151;
	or.b32  	%r153, %r149, %r152;
	and.b32  	%r154, %r153, %r147;
	shl.b32 	%r155, %r3, %r151;
	shr.u32 	%r156, %r198, %r148;
	or.b32  	%r157, %r155, %r156;
	and.b32  	%r158, %r154, %r157;
	mov.u32 	%r159, 30;
	sub.s32 	%r160, %r159, %r213;
	and.b32  	%r161, %r160, 31;
	shl.b32 	%r162, %r197, %r161;
	add.s32 	%r163, %r213, 2;
	and.b32  	%r164, %r163, 31;
	shr.u32 	%r165, %r3, %r164;
	or.b32  	%r166, %r162, %r165;
	and.b32  	%r167, %r166, %r158;
	shl.b32 	%r168, %r3, %r164;
	shr.u32 	%r169, %r198, %r161;
	or.b32  	%r170, %r168, %r169;
	and.b32  	%r171, %r167, %r170;
	mov.u32 	%r172, 29;
	sub.s32 	%r173, %r172, %r213;
	and.b32  	%r174, %r173, 31;
	shl.b32 	%r175, %r197, %r174;
	add.s32 	%r176, %r213, 3;
	and.b32  	%r177, %r176, 31;
	shr.u32 	%r178, %r3, %r177;
	or.b32  	%r179, %r175, %r178;
	and.b32  	%r180, %r179, %r171;
	shl.b32 	%r181, %r3, %r177;
	shr.u32 	%r182, %r198, %r174;
	or.b32  	%r183, %r181, %r182;
	and.b32  	%r219, %r180, %r183;
	add.s32 	%r213, %r213, 4;
	add.s32 	%r211, %r211, -4;
	setp.ne.s32 	%p16, %r211, 0;
	@%p16 bra 	$L__BB10_15;

$L__BB10_16:
	setp.eq.s32 	%p17, %r208, 0;
	@%p17 bra 	$L__BB10_19;

	neg.s32 	%r215, %r213;

$L__BB10_18:
	.pragma "nounroll";
	and.b32  	%r184, %r215, 31;
	shl.b32 	%r185, %r197, %r184;
	and.b32  	%r186, %r213, 31;
	shr.u32 	%r187, %r3, %r186;
	or.b32  	%r188, %r185, %r187;
	and.b32  	%r189, %r188, %r219;
	shl.b32 	%r190, %r3, %r186;
	shr.u32 	%r191, %r198, %r184;
	or.b32  	%r192, %r190, %r191;
	and.b32  	%r219, %r189, %r192;
	add.s32 	%r213, %r213, 1;
	add.s32 	%r215, %r215, -1;
	add.s32 	%r208, %r208, -1;
	setp.ne.s32 	%p18, %r208, 0;
	@%p18 bra 	$L__BB10_18;

$L__BB10_19:
	selp.b32 	%r193, %r53, -1, %p3;
	selp.b32 	%r194, %r52, %r193, %p2;
	setp.eq.s16 	%p21, %rs1, 0;
	selp.b32 	%r195, -1, %r194, %p21;
	and.b32  	%r196, %r219, %r195;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r196;

$L__BB10_20:
	ret;

}
	// .globl	morphoErodeVer
.entry morphoErodeVer(
	.param .u64 .ptr .global .align 4 morphoErodeVer_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeVer_param_1,
	.param .u32 morphoErodeVer_param_2,
	.param .u32 morphoErodeVer_param_3,
	.param .u32 morphoErodeVer_param_4,
	.param .u8 morphoErodeVer_param_5,
	.param .u32 morphoErodeVer_param_6
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<28>;


	ld.param.s8 	%rs1, [morphoErodeVer_param_5];
	ld.param.u64 	%rd11, [morphoErodeVer_param_0];
	ld.param.u64 	%rd12, [morphoErodeVer_param_1];
	ld.param.u32 	%r22, [morphoErodeVer_param_2];
	ld.param.u32 	%r23, [morphoErodeVer_param_3];
	ld.param.u32 	%r24, [morphoErodeVer_param_4];
	ld.param.u32 	%r25, [morphoErodeVer_param_6];
	mov.b32 	%r26, %envreg3;
	mov.u32 	%r27, %ctaid.x;
	mov.u32 	%r28, %ntid.x;
	mov.u32 	%r29, %tid.x;
	add.s32 	%r30, %r29, %r26;
	mad.lo.s32 	%r1, %r28, %r27, %r30;
	mov.u32 	%r31, %ctaid.y;
	mov.u32 	%r32, %ntid.y;
	mov.u32 	%r33, %tid.y;
	mov.b32 	%r34, %envreg4;
	add.s32 	%r35, %r33, %r34;
	mad.lo.s32 	%r2, %r32, %r31, %r35;
	setp.ge.s32 	%p1, %r2, %r24;
	setp.ge.s32 	%p2, %r1, %r23;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB11_9;

	mad.lo.s32 	%r36, %r2, %r23, %r1;
	cvt.u64.u32 	%rd1, %r36;
	mul.wide.u32 	%rd13, %r36, 4;
	add.s64 	%rd14, %rd11, %rd13;
	ld.global.u32 	%r65, [%rd14];
	sub.s32 	%r37, %r2, %r22;
	max.s32 	%r4, %r37, 0;
	sub.s32 	%r38, %r24, %r25;
	setp.gt.s32 	%p4, %r38, %r2;
	add.s32 	%r39, %r2, %r25;
	add.s32 	%r40, %r24, -1;
	selp.b32 	%r5, %r39, %r40, %p4;
	setp.lt.s32 	%p5, %r5, %r4;
	@%p5 bra 	$L__BB11_8;

	cvt.s64.s32 	%rd2, %r1;
	add.s32 	%r42, %r5, 1;
	sub.s32 	%r43, %r42, %r4;
	and.b32  	%r59, %r43, 3;
	setp.eq.s32 	%p6, %r59, 0;
	mov.u32 	%r60, %r4;
	@%p6 bra 	$L__BB11_5;

	mul.lo.s32 	%r44, %r23, %r4;
	cvt.s64.s32 	%rd15, %r44;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd26, %rd11, %rd17;
	mul.wide.s32 	%rd4, %r23, 4;
	mov.u32 	%r60, %r4;

$L__BB11_4:
	.pragma "nounroll";
	ld.global.u32 	%r45, [%rd26];
	and.b32  	%r65, %r45, %r65;
	add.s32 	%r60, %r60, 1;
	add.s64 	%rd26, %rd26, %rd4;
	add.s32 	%r59, %r59, -1;
	setp.ne.s32 	%p7, %r59, 0;
	@%p7 bra 	$L__BB11_4;

$L__BB11_5:
	sub.s32 	%r46, %r5, %r4;
	setp.lt.u32 	%p8, %r46, 3;
	@%p8 bra 	$L__BB11_8;

	add.s32 	%r63, %r60, -1;
	mul.lo.s32 	%r47, %r60, %r23;
	cvt.s64.s32 	%rd18, %r47;
	add.s64 	%rd19, %rd18, %rd2;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd27, %rd11, %rd20;
	mul.wide.s32 	%rd8, %r23, 4;

$L__BB11_7:
	ld.global.u32 	%r48, [%rd27];
	and.b32  	%r49, %r48, %r65;
	add.s64 	%rd21, %rd27, %rd8;
	ld.global.u32 	%r50, [%rd21];
	and.b32  	%r51, %r50, %r49;
	add.s64 	%rd22, %rd21, %rd8;
	ld.global.u32 	%r52, [%rd22];
	and.b32  	%r53, %r52, %r51;
	add.s64 	%rd23, %rd22, %rd8;
	add.s64 	%rd27, %rd23, %rd8;
	ld.global.u32 	%r54, [%rd23];
	and.b32  	%r65, %r54, %r53;
	add.s32 	%r63, %r63, 4;
	setp.lt.s32 	%p9, %r63, %r5;
	@%p9 bra 	$L__BB11_7;

$L__BB11_8:
	sub.s32 	%r55, %r24, %r2;
	setp.le.s32 	%p10, %r55, %r25;
	setp.lt.s32 	%p11, %r2, %r22;
	or.pred  	%p12, %p11, %p10;
	setp.ne.s16 	%p13, %rs1, 0;
	and.pred  	%p14, %p13, %p12;
	selp.b32 	%r56, 0, %r65, %p14;
	shl.b64 	%rd24, %rd1, 2;
	add.s64 	%rd25, %rd12, %rd24;
	st.global.u32 	[%rd25], %r56;

$L__BB11_9:
	ret;

}
	// .globl	kernel_HistogramRectAllChannels
.entry kernel_HistogramRectAllChannels(
	.param .u64 .ptr .global .align 8 kernel_HistogramRectAllChannels_param_0,
	.param .u32 kernel_HistogramRectAllChannels_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannels_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd5, [kernel_HistogramRectAllChannels_param_0];
	ld.param.u32 	%r8, [kernel_HistogramRectAllChannels_param_1];
	ld.param.u64 	%rd6, [kernel_HistogramRectAllChannels_param_2];
	mov.b32 	%r9, %envreg3;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %tid.x;
	add.s32 	%r12, %r11, %r9;
	mad.lo.s32 	%r2, %r1, %r10, %r12;
	and.b32  	%r3, %r2, 255;
	shr.u32 	%r13, %r8, 1;
	and.b32  	%r4, %r13, 536870911;
	setp.le.u32 	%p1, %r4, %r2;
	@%p1 bra 	$L__BB12_3;

	cvt.s64.s32 	%rd27, %r2;
	or.b32  	%r5, %r3, 65536;
	or.b32  	%r6, %r3, 131072;
	or.b32  	%r7, %r3, 196608;
	mov.b32 	%r14, %envreg6;
	mul.lo.s32 	%r15, %r14, %r1;
	cvt.s64.s32 	%rd2, %r15;

$L__BB12_2:
	and.b64  	%rd7, %rd27, 4294967295;
	shl.b64 	%rd8, %rd27, 3;
	and.b64  	%rd9, %rd8, 34359738360;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	mul.wide.u16 	%r16, %rs1, 256;
	or.b32  	%r17, %r16, %r3;
	mul.wide.u32 	%rd11, %r17, 4;
	add.s64 	%rd12, %rd6, %rd11;
	ld.global.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [%rd10+4];
	red.global.add.u32 	[%rd12], 1;
	mul.wide.u16 	%r19, %rs5, 256;
	or.b32  	%r20, %r19, %r3;
	mul.wide.u32 	%rd13, %r20, 4;
	add.s64 	%rd14, %rd6, %rd13;
	red.global.add.u32 	[%rd14], 1;
	mul.wide.u16 	%r22, %rs2, 256;
	add.s32 	%r23, %r5, %r22;
	mul.wide.u32 	%rd15, %r23, 4;
	add.s64 	%rd16, %rd6, %rd15;
	red.global.add.u32 	[%rd16], 1;
	mul.wide.u16 	%r25, %rs6, 256;
	add.s32 	%r26, %r5, %r25;
	mul.wide.u32 	%rd17, %r26, 4;
	add.s64 	%rd18, %rd6, %rd17;
	red.global.add.u32 	[%rd18], 1;
	mul.wide.u16 	%r28, %rs3, 256;
	add.s32 	%r29, %r6, %r28;
	mul.wide.u32 	%rd19, %r29, 4;
	add.s64 	%rd20, %rd6, %rd19;
	red.global.add.u32 	[%rd20], 1;
	mul.wide.u16 	%r31, %rs7, 256;
	add.s32 	%r32, %r6, %r31;
	mul.wide.u32 	%rd21, %r32, 4;
	add.s64 	%rd22, %rd6, %rd21;
	red.global.add.u32 	[%rd22], 1;
	mul.wide.u16 	%r34, %rs4, 256;
	add.s32 	%r35, %r7, %r34;
	mul.wide.u32 	%rd23, %r35, 4;
	add.s64 	%rd24, %rd6, %rd23;
	red.global.add.u32 	[%rd24], 1;
	mul.wide.u16 	%r37, %rs8, 256;
	add.s32 	%r38, %r7, %r37;
	mul.wide.u32 	%rd25, %r38, 4;
	add.s64 	%rd26, %rd6, %rd25;
	red.global.add.u32 	[%rd26], 1;
	add.s64 	%rd27, %rd7, %rd2;
	cvt.u32.u64 	%r40, %rd27;
	setp.gt.u32 	%p2, %r4, %r40;
	@%p2 bra 	$L__BB12_2;

$L__BB12_3:
	ret;

}
	// .globl	kernel_HistogramRectOneChannel
.entry kernel_HistogramRectOneChannel(
	.param .u64 .ptr .global .align 8 kernel_HistogramRectOneChannel_param_0,
	.param .u32 kernel_HistogramRectOneChannel_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannel_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd5, [kernel_HistogramRectOneChannel_param_0];
	ld.param.u32 	%r5, [kernel_HistogramRectOneChannel_param_1];
	ld.param.u64 	%rd6, [kernel_HistogramRectOneChannel_param_2];
	mov.b32 	%r6, %envreg3;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %tid.x;
	add.s32 	%r9, %r8, %r6;
	mad.lo.s32 	%r2, %r1, %r7, %r9;
	shr.u32 	%r3, %r5, 3;
	setp.le.u32 	%p1, %r3, %r2;
	@%p1 bra 	$L__BB13_3;

	and.b32  	%r4, %r2, 255;
	cvt.s64.s32 	%rd27, %r2;
	mov.b32 	%r10, %envreg6;
	mul.lo.s32 	%r11, %r10, %r1;
	cvt.s64.s32 	%rd2, %r11;

$L__BB13_2:
	and.b64  	%rd7, %rd27, 4294967295;
	shl.b64 	%rd8, %rd27, 3;
	and.b64  	%rd9, %rd8, 34359738360;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	mul.wide.u16 	%r12, %rs1, 256;
	or.b32  	%r13, %r12, %r4;
	mul.wide.u32 	%rd11, %r13, 4;
	add.s64 	%rd12, %rd6, %rd11;
	ld.global.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [%rd10+4];
	red.global.add.u32 	[%rd12], 1;
	mul.wide.u16 	%r15, %rs2, 256;
	or.b32  	%r16, %r15, %r4;
	mul.wide.u32 	%rd13, %r16, 4;
	add.s64 	%rd14, %rd6, %rd13;
	red.global.add.u32 	[%rd14], 1;
	mul.wide.u16 	%r18, %rs3, 256;
	or.b32  	%r19, %r18, %r4;
	mul.wide.u32 	%rd15, %r19, 4;
	add.s64 	%rd16, %rd6, %rd15;
	red.global.add.u32 	[%rd16], 1;
	mul.wide.u16 	%r21, %rs4, 256;
	or.b32  	%r22, %r21, %r4;
	mul.wide.u32 	%rd17, %r22, 4;
	add.s64 	%rd18, %rd6, %rd17;
	red.global.add.u32 	[%rd18], 1;
	mul.wide.u16 	%r24, %rs5, 256;
	or.b32  	%r25, %r24, %r4;
	mul.wide.u32 	%rd19, %r25, 4;
	add.s64 	%rd20, %rd6, %rd19;
	red.global.add.u32 	[%rd20], 1;
	mul.wide.u16 	%r27, %rs6, 256;
	or.b32  	%r28, %r27, %r4;
	mul.wide.u32 	%rd21, %r28, 4;
	add.s64 	%rd22, %rd6, %rd21;
	red.global.add.u32 	[%rd22], 1;
	mul.wide.u16 	%r30, %rs7, 256;
	or.b32  	%r31, %r30, %r4;
	mul.wide.u32 	%rd23, %r31, 4;
	add.s64 	%rd24, %rd6, %rd23;
	red.global.add.u32 	[%rd24], 1;
	mul.wide.u16 	%r33, %rs8, 256;
	or.b32  	%r34, %r33, %r4;
	mul.wide.u32 	%rd25, %r34, 4;
	add.s64 	%rd26, %rd6, %rd25;
	red.global.add.u32 	[%rd26], 1;
	add.s64 	%rd27, %rd7, %rd2;
	cvt.u32.u64 	%r36, %rd27;
	setp.gt.u32 	%p2, %r3, %r36;
	@%p2 bra 	$L__BB13_2;

$L__BB13_3:
	ret;

}
	// .globl	kernel_HistogramRectAllChannelsReduction
.entry kernel_HistogramRectAllChannelsReduction(
	.param .u32 kernel_HistogramRectAllChannelsReduction_param_0,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannelsReduction_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannelsReduction_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<85>;
	.reg .b64 	%rd<12>;
	// demoted variable
	.shared .align 4 .b8 kernel_HistogramRectAllChannelsReduction_$_localHist[1024];

	ld.param.u64 	%rd4, [kernel_HistogramRectAllChannelsReduction_param_1];
	ld.param.u64 	%rd5, [kernel_HistogramRectAllChannelsReduction_param_2];
	mov.b32 	%r27, %envreg0;
	mov.u32 	%r28, %ctaid.x;
	add.s32 	%r29, %r27, %r28;
	cvt.s64.s32 	%rd1, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.s64.s32 	%rd2, %r30;
	setp.gt.u32 	%p1, %r30, 255;
	mov.u32 	%r76, 0;
	@%p1 bra 	$L__BB14_3;

	cvt.u32.u64 	%r32, %rd2;
	cvt.u32.u64 	%r33, %rd1;
	add.s32 	%r74, %r32, -256;
	shl.b32 	%r34, %r33, 8;
	and.b32  	%r35, %r34, 65280;
	add.s32 	%r36, %r32, %r35;
	and.b32  	%r37, %r34, -65536;
	add.s32 	%r73, %r36, %r37;
	mov.u32 	%r76, 0;

$L__BB14_2:
	mul.wide.u32 	%rd6, %r73, 4;
	add.s64 	%rd7, %rd4, %rd6;
	ld.global.u32 	%r38, [%rd7];
	add.s32 	%r76, %r38, %r76;
	add.s32 	%r73, %r73, 256;
	add.s32 	%r74, %r74, 256;
	setp.gt.u32 	%p2, %r74, -257;
	@%p2 bra 	$L__BB14_2;

$L__BB14_3:
	shl.b64 	%rd8, %rd2, 2;
	mov.u64 	%rd9, kernel_HistogramRectAllChannelsReduction_$_localHist;
	add.s64 	%rd3, %rd9, %rd8;
	st.shared.u32 	[%rd3], %r76;
	bar.sync 	0;
	cvt.u32.u64 	%r39, %rd2;
	setp.gt.u32 	%p3, %r39, 127;
	@%p3 bra 	$L__BB14_5;

	ld.shared.u32 	%r76, [%rd3+512];

$L__BB14_5:
	bar.sync 	0;
	@%p3 bra 	$L__BB14_7;

	ld.shared.u32 	%r41, [%rd3];
	add.s32 	%r42, %r41, %r76;
	st.shared.u32 	[%rd3], %r42;

$L__BB14_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r39, 63;
	@%p5 bra 	$L__BB14_9;

	ld.shared.u32 	%r76, [%rd3+256];

$L__BB14_9:
	bar.sync 	0;
	@%p5 bra 	$L__BB14_11;

	ld.shared.u32 	%r45, [%rd3];
	add.s32 	%r46, %r45, %r76;
	st.shared.u32 	[%rd3], %r46;

$L__BB14_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r39, 31;
	@%p7 bra 	$L__BB14_13;

	ld.shared.u32 	%r76, [%rd3+128];

$L__BB14_13:
	bar.sync 	0;
	@%p7 bra 	$L__BB14_15;

	ld.shared.u32 	%r49, [%rd3];
	add.s32 	%r50, %r49, %r76;
	st.shared.u32 	[%rd3], %r50;

$L__BB14_15:
	bar.sync 	0;
	setp.gt.u32 	%p9, %r39, 15;
	@%p9 bra 	$L__BB14_17;

	ld.shared.u32 	%r76, [%rd3+64];

$L__BB14_17:
	bar.sync 	0;
	@%p9 bra 	$L__BB14_19;

	ld.shared.u32 	%r53, [%rd3];
	add.s32 	%r54, %r53, %r76;
	st.shared.u32 	[%rd3], %r54;

$L__BB14_19:
	bar.sync 	0;
	setp.gt.u32 	%p11, %r39, 7;
	@%p11 bra 	$L__BB14_21;

	ld.shared.u32 	%r76, [%rd3+32];

$L__BB14_21:
	bar.sync 	0;
	@%p11 bra 	$L__BB14_23;

	ld.shared.u32 	%r57, [%rd3];
	add.s32 	%r58, %r57, %r76;
	st.shared.u32 	[%rd3], %r58;

$L__BB14_23:
	bar.sync 	0;
	setp.gt.u32 	%p13, %r39, 3;
	@%p13 bra 	$L__BB14_25;

	ld.shared.u32 	%r76, [%rd3+16];

$L__BB14_25:
	bar.sync 	0;
	@%p13 bra 	$L__BB14_27;

	ld.shared.u32 	%r61, [%rd3];
	add.s32 	%r62, %r61, %r76;
	st.shared.u32 	[%rd3], %r62;

$L__BB14_27:
	bar.sync 	0;
	setp.gt.u32 	%p15, %r39, 1;
	@%p15 bra 	$L__BB14_29;

	ld.shared.u32 	%r76, [%rd3+8];

$L__BB14_29:
	bar.sync 	0;
	@%p15 bra 	$L__BB14_31;

	ld.shared.u32 	%r65, [%rd3];
	add.s32 	%r66, %r65, %r76;
	st.shared.u32 	[%rd3], %r66;

$L__BB14_31:
	bar.sync 	0;
	setp.ne.s32 	%p17, %r39, 0;
	@%p17 bra 	$L__BB14_33;

	ld.shared.u32 	%r76, [%rd3+4];

$L__BB14_33:
	bar.sync 	0;
	@%p17 bra 	$L__BB14_35;

	ld.shared.u32 	%r69, [%rd3];
	add.s32 	%r70, %r69, %r76;
	st.shared.u32 	[%rd3], %r70;

$L__BB14_35:
	bar.sync 	0;
	@%p17 bra 	$L__BB14_37;

	ld.shared.u32 	%r72, [kernel_HistogramRectAllChannelsReduction_$_localHist];
	shl.b64 	%rd10, %rd1, 2;
	add.s64 	%rd11, %rd5, %rd10;
	st.global.u32 	[%rd11], %r72;

$L__BB14_37:
	ret;

}
	// .globl	kernel_HistogramRectOneChannelReduction
.entry kernel_HistogramRectOneChannelReduction(
	.param .u32 kernel_HistogramRectOneChannelReduction_param_0,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannelReduction_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannelReduction_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<93>;
	.reg .b64 	%rd<20>;
	// demoted variable
	.shared .align 4 .b8 kernel_HistogramRectOneChannelReduction_$_localHist[1024];

	ld.param.u64 	%rd9, [kernel_HistogramRectOneChannelReduction_param_1];
	ld.param.u64 	%rd10, [kernel_HistogramRectOneChannelReduction_param_2];
	mov.b32 	%r37, %envreg0;
	mov.u32 	%r38, %ctaid.x;
	add.s32 	%r39, %r37, %r38;
	cvt.s64.s32 	%rd1, %r39;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 255;
	mov.u32 	%r84, 0;
	@%p1 bra 	$L__BB15_7;

	cvt.u32.u64 	%r42, %rd1;
	shl.b32 	%r43, %r42, 8;
	and.b32  	%r2, %r43, 65280;
	mov.u32 	%r84, 0;
	max.s32 	%r44, %r1, 0;
	add.s32 	%r45, %r44, 255;
	sub.s32 	%r46, %r45, %r1;
	shr.u32 	%r3, %r46, 8;
	add.s32 	%r47, %r3, 1;
	and.b32  	%r83, %r47, 3;
	setp.lt.u32 	%p2, %r46, 768;
	mov.u32 	%r80, %r1;
	@%p2 bra 	$L__BB15_4;

	add.s32 	%r49, %r1, %r2;
	mul.wide.s32 	%rd11, %r49, 4;
	add.s64 	%rd12, %rd9, %rd11;
	add.s64 	%rd18, %rd12, 2048;
	sub.s32 	%r76, %r3, %r83;
	mov.u32 	%r84, 0;
	mov.u32 	%r80, %r1;

$L__BB15_3:
	ld.global.u32 	%r50, [%rd18+-2048];
	add.s32 	%r51, %r50, %r84;
	ld.global.u32 	%r52, [%rd18+-1024];
	add.s32 	%r53, %r52, %r51;
	ld.global.u32 	%r54, [%rd18];
	add.s32 	%r55, %r54, %r53;
	ld.global.u32 	%r56, [%rd18+1024];
	add.s32 	%r84, %r56, %r55;
	add.s32 	%r80, %r80, 1024;
	add.s64 	%rd18, %rd18, 4096;
	add.s32 	%r76, %r76, -4;
	setp.ne.s32 	%p3, %r76, -1;
	@%p3 bra 	$L__BB15_3;

$L__BB15_4:
	setp.eq.s32 	%p4, %r83, 0;
	@%p4 bra 	$L__BB15_7;

	add.s32 	%r57, %r80, %r2;
	mul.wide.s32 	%rd13, %r57, 4;
	add.s64 	%rd19, %rd9, %rd13;

$L__BB15_6:
	.pragma "nounroll";
	ld.global.u32 	%r58, [%rd19];
	add.s32 	%r84, %r58, %r84;
	add.s64 	%rd19, %rd19, 1024;
	add.s32 	%r83, %r83, -1;
	setp.ne.s32 	%p5, %r83, 0;
	@%p5 bra 	$L__BB15_6;

$L__BB15_7:
	mul.wide.s32 	%rd14, %r1, 4;
	mov.u64 	%rd15, kernel_HistogramRectOneChannelReduction_$_localHist;
	add.s64 	%rd8, %rd15, %rd14;
	st.shared.u32 	[%rd8], %r84;
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB15_9;

	ld.shared.u32 	%r84, [%rd8+512];

$L__BB15_9:
	bar.sync 	0;
	@%p6 bra 	$L__BB15_11;

	ld.shared.u32 	%r59, [%rd8];
	add.s32 	%r60, %r59, %r84;
	st.shared.u32 	[%rd8], %r60;

$L__BB15_11:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 63;
	@%p8 bra 	$L__BB15_13;

	ld.shared.u32 	%r84, [%rd8+256];

$L__BB15_13:
	bar.sync 	0;
	@%p8 bra 	$L__BB15_15;

	ld.shared.u32 	%r61, [%rd8];
	add.s32 	%r62, %r61, %r84;
	st.shared.u32 	[%rd8], %r62;

$L__BB15_15:
	bar.sync 	0;
	setp.gt.u32 	%p10, %r1, 31;
	@%p10 bra 	$L__BB15_17;

	ld.shared.u32 	%r84, [%rd8+128];

$L__BB15_17:
	bar.sync 	0;
	@%p10 bra 	$L__BB15_19;

	ld.shared.u32 	%r63, [%rd8];
	add.s32 	%r64, %r63, %r84;
	st.shared.u32 	[%rd8], %r64;

$L__BB15_19:
	bar.sync 	0;
	setp.gt.u32 	%p12, %r1, 15;
	@%p12 bra 	$L__BB15_21;

	ld.shared.u32 	%r84, [%rd8+64];

$L__BB15_21:
	bar.sync 	0;
	@%p12 bra 	$L__BB15_23;

	ld.shared.u32 	%r65, [%rd8];
	add.s32 	%r66, %r65, %r84;
	st.shared.u32 	[%rd8], %r66;

$L__BB15_23:
	bar.sync 	0;
	setp.gt.u32 	%p14, %r1, 7;
	@%p14 bra 	$L__BB15_25;

	ld.shared.u32 	%r84, [%rd8+32];

$L__BB15_25:
	bar.sync 	0;
	@%p14 bra 	$L__BB15_27;

	ld.shared.u32 	%r67, [%rd8];
	add.s32 	%r68, %r67, %r84;
	st.shared.u32 	[%rd8], %r68;

$L__BB15_27:
	bar.sync 	0;
	setp.gt.u32 	%p16, %r1, 3;
	@%p16 bra 	$L__BB15_29;

	ld.shared.u32 	%r84, [%rd8+16];

$L__BB15_29:
	bar.sync 	0;
	@%p16 bra 	$L__BB15_31;

	ld.shared.u32 	%r69, [%rd8];
	add.s32 	%r70, %r69, %r84;
	st.shared.u32 	[%rd8], %r70;

$L__BB15_31:
	bar.sync 	0;
	setp.gt.u32 	%p18, %r1, 1;
	@%p18 bra 	$L__BB15_33;

	ld.shared.u32 	%r84, [%rd8+8];

$L__BB15_33:
	bar.sync 	0;
	@%p18 bra 	$L__BB15_35;

	ld.shared.u32 	%r71, [%rd8];
	add.s32 	%r72, %r71, %r84;
	st.shared.u32 	[%rd8], %r72;

$L__BB15_35:
	bar.sync 	0;
	setp.ne.s32 	%p20, %r1, 0;
	@%p20 bra 	$L__BB15_37;

	ld.shared.u32 	%r84, [%rd8+4];

$L__BB15_37:
	bar.sync 	0;
	@%p20 bra 	$L__BB15_39;

	ld.shared.u32 	%r73, [%rd8];
	add.s32 	%r74, %r73, %r84;
	st.shared.u32 	[%rd8], %r74;

$L__BB15_39:
	bar.sync 	0;
	@%p20 bra 	$L__BB15_41;

	ld.shared.u32 	%r75, [kernel_HistogramRectOneChannelReduction_$_localHist];
	shl.b64 	%rd16, %rd1, 2;
	add.s64 	%rd17, %rd10, %rd16;
	st.global.u32 	[%rd17], %r75;

$L__BB15_41:
	ret;

}
	// .globl	kernel_ThresholdRectToPix
.entry kernel_ThresholdRectToPix(
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_0,
	.param .u32 kernel_ThresholdRectToPix_param_1,
	.param .u32 kernel_ThresholdRectToPix_param_2,
	.param .u32 kernel_ThresholdRectToPix_param_3,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_4,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_5,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_6
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<132>;
	.reg .b16 	%rs<65>;
	.reg .b32 	%r<293>;
	.reg .b64 	%rd<18>;


	ld.param.u32 	%r95, [kernel_ThresholdRectToPix_param_1];
	ld.param.u32 	%r94, [kernel_ThresholdRectToPix_param_3];
	ld.param.u64 	%rd7, [kernel_ThresholdRectToPix_param_4];
	ld.param.u64 	%rd8, [kernel_ThresholdRectToPix_param_5];
	ld.global.u32 	%r1, [%rd7];
	ld.global.u32 	%r2, [%rd8];
	ld.global.u32 	%r3, [%rd7+4];
	ld.global.u32 	%r4, [%rd8+4];
	ld.global.u32 	%r5, [%rd7+8];
	ld.global.u32 	%r6, [%rd8+8];
	ld.global.u32 	%r7, [%rd7+12];
	ld.global.u32 	%r8, [%rd8+12];
	mov.u32 	%r96, %ctaid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r97, %tid.x;
	mov.b32 	%r98, %envreg3;
	add.s32 	%r99, %r97, %r98;
	mad.lo.s32 	%r257, %r9, %r96, %r99;
	mul.lo.s32 	%r11, %r94, %r95;
	setp.le.u32 	%p1, %r11, %r257;
	@%p1 bra 	$L__BB16_69;

	cvt.s64.s32 	%rd17, %r257;
	mov.b32 	%r100, %envreg6;
	mul.lo.s32 	%r101, %r100, %r9;
	cvt.s64.s32 	%rd2, %r101;

$L__BB16_2:
	ld.param.u32 	%r216, [kernel_ThresholdRectToPix_param_2];
	ld.param.u32 	%r215, [kernel_ThresholdRectToPix_param_3];
	div.u32 	%r105, %r257, %r215;
	mul.lo.s32 	%r13, %r105, %r216;
	mul.lo.s32 	%r106, %r105, %r215;
	sub.s32 	%r107, %r257, %r106;
	shl.b32 	%r14, %r107, 5;
	mov.u32 	%r258, 0;
	mov.u32 	%r259, %r258;
	mov.u32 	%r261, %r258;

$L__BB16_3:
	ld.param.u64 	%rd15, [kernel_ThresholdRectToPix_param_0];
	shl.b32 	%r108, %r259, 3;
	add.s32 	%r109, %r13, %r108;
	add.s32 	%r110, %r109, %r14;
	mul.wide.s32 	%rd9, %r110, 4;
	add.s64 	%rd10, %rd15, %rd9;
	ld.global.v4.u8 	{%rs33, %rs34, %rs35, %rs36}, [%rd10];
	ld.global.v4.u8 	{%rs37, %rs38, %rs39, %rs40}, [%rd10+4];
	ld.global.v4.u8 	{%rs41, %rs42, %rs43, %rs44}, [%rd10+8];
	ld.global.v4.u8 	{%rs45, %rs46, %rs47, %rs48}, [%rd10+12];
	ld.global.v4.u8 	{%rs49, %rs50, %rs51, %rs52}, [%rd10+16];
	ld.global.v4.u8 	{%rs53, %rs54, %rs55, %rs56}, [%rd10+20];
	ld.global.v4.u8 	{%rs57, %rs58, %rs59, %rs60}, [%rd10+24];
	ld.global.v4.u8 	{%rs61, %rs62, %rs63, %rs64}, [%rd10+28];
	setp.lt.s32 	%p2, %r2, 0;
	@%p2 bra 	$L__BB16_5;

	mov.u32 	%r224, -2147483648;
	shr.u32 	%r223, %r224, %r258;
	setp.eq.s32 	%p3, %r2, 0;
	cvt.u32.u16 	%r112, %rs33;
	and.b32  	%r113, %r112, 255;
	setp.lt.s32 	%p4, %r1, %r113;
	xor.pred  	%p5, %p4, %p3;
	selp.b32 	%r114, 0, %r223, %p5;
	or.b32  	%r261, %r261, %r114;

$L__BB16_5:
	setp.lt.s32 	%p6, %r4, 0;
	@%p6 bra 	$L__BB16_7;

	mov.u32 	%r222, -2147483648;
	shr.u32 	%r221, %r222, %r258;
	setp.eq.s32 	%p7, %r4, 0;
	cvt.u32.u16 	%r115, %rs34;
	and.b32  	%r116, %r115, 255;
	setp.lt.s32 	%p8, %r3, %r116;
	xor.pred  	%p9, %p8, %p7;
	selp.b32 	%r117, 0, %r221, %p9;
	or.b32  	%r261, %r261, %r117;

$L__BB16_7:
	setp.lt.s32 	%p10, %r6, 0;
	@%p10 bra 	$L__BB16_9;

	mov.u32 	%r220, -2147483648;
	shr.u32 	%r219, %r220, %r258;
	setp.eq.s32 	%p11, %r6, 0;
	cvt.u32.u16 	%r118, %rs35;
	and.b32  	%r119, %r118, 255;
	setp.lt.s32 	%p12, %r5, %r119;
	xor.pred  	%p13, %p12, %p11;
	selp.b32 	%r120, 0, %r219, %p13;
	or.b32  	%r261, %r261, %r120;

$L__BB16_9:
	setp.lt.s32 	%p14, %r8, 0;
	@%p14 bra 	$L__BB16_11;

	mov.u32 	%r218, -2147483648;
	shr.u32 	%r217, %r218, %r258;
	setp.eq.s32 	%p15, %r8, 0;
	cvt.u32.u16 	%r121, %rs36;
	and.b32  	%r122, %r121, 255;
	setp.lt.s32 	%p16, %r7, %r122;
	xor.pred  	%p17, %p16, %p15;
	selp.b32 	%r123, 0, %r217, %p17;
	or.b32  	%r261, %r261, %r123;

$L__BB16_11:
	@%p2 bra 	$L__BB16_13;

	mov.u32 	%r232, 1073741824;
	shr.u32 	%r231, %r232, %r258;
	setp.eq.s32 	%p19, %r2, 0;
	cvt.u32.u16 	%r125, %rs37;
	and.b32  	%r126, %r125, 255;
	setp.lt.s32 	%p20, %r1, %r126;
	xor.pred  	%p21, %p20, %p19;
	selp.b32 	%r127, 0, %r231, %p21;
	or.b32  	%r261, %r261, %r127;

$L__BB16_13:
	@%p6 bra 	$L__BB16_15;

	mov.u32 	%r230, 1073741824;
	shr.u32 	%r229, %r230, %r258;
	setp.eq.s32 	%p23, %r4, 0;
	cvt.u32.u16 	%r128, %rs38;
	and.b32  	%r129, %r128, 255;
	setp.lt.s32 	%p24, %r3, %r129;
	xor.pred  	%p25, %p24, %p23;
	selp.b32 	%r130, 0, %r229, %p25;
	or.b32  	%r261, %r261, %r130;

$L__BB16_15:
	@%p10 bra 	$L__BB16_17;

	mov.u32 	%r228, 1073741824;
	shr.u32 	%r227, %r228, %r258;
	setp.eq.s32 	%p27, %r6, 0;
	cvt.u32.u16 	%r131, %rs39;
	and.b32  	%r132, %r131, 255;
	setp.lt.s32 	%p28, %r5, %r132;
	xor.pred  	%p29, %p28, %p27;
	selp.b32 	%r133, 0, %r227, %p29;
	or.b32  	%r261, %r261, %r133;

$L__BB16_17:
	@%p14 bra 	$L__BB16_19;

	mov.u32 	%r226, 1073741824;
	shr.u32 	%r225, %r226, %r258;
	setp.eq.s32 	%p31, %r8, 0;
	cvt.u32.u16 	%r134, %rs40;
	and.b32  	%r135, %r134, 255;
	setp.lt.s32 	%p32, %r7, %r135;
	xor.pred  	%p33, %p32, %p31;
	selp.b32 	%r136, 0, %r225, %p33;
	or.b32  	%r261, %r261, %r136;

$L__BB16_19:
	@%p2 bra 	$L__BB16_21;

	mov.u32 	%r240, 536870912;
	shr.u32 	%r239, %r240, %r258;
	setp.eq.s32 	%p35, %r2, 0;
	cvt.u32.u16 	%r138, %rs41;
	and.b32  	%r139, %r138, 255;
	setp.lt.s32 	%p36, %r1, %r139;
	xor.pred  	%p37, %p36, %p35;
	selp.b32 	%r140, 0, %r239, %p37;
	or.b32  	%r261, %r261, %r140;

$L__BB16_21:
	@%p6 bra 	$L__BB16_23;

	mov.u32 	%r238, 536870912;
	shr.u32 	%r237, %r238, %r258;
	setp.eq.s32 	%p39, %r4, 0;
	cvt.u32.u16 	%r141, %rs42;
	and.b32  	%r142, %r141, 255;
	setp.lt.s32 	%p40, %r3, %r142;
	xor.pred  	%p41, %p40, %p39;
	selp.b32 	%r143, 0, %r237, %p41;
	or.b32  	%r261, %r261, %r143;

$L__BB16_23:
	@%p10 bra 	$L__BB16_25;

	mov.u32 	%r236, 536870912;
	shr.u32 	%r235, %r236, %r258;
	setp.eq.s32 	%p43, %r6, 0;
	cvt.u32.u16 	%r144, %rs43;
	and.b32  	%r145, %r144, 255;
	setp.lt.s32 	%p44, %r5, %r145;
	xor.pred  	%p45, %p44, %p43;
	selp.b32 	%r146, 0, %r235, %p45;
	or.b32  	%r261, %r261, %r146;

$L__BB16_25:
	@%p14 bra 	$L__BB16_27;

	mov.u32 	%r234, 536870912;
	shr.u32 	%r233, %r234, %r258;
	setp.eq.s32 	%p47, %r8, 0;
	cvt.u32.u16 	%r147, %rs44;
	and.b32  	%r148, %r147, 255;
	setp.lt.s32 	%p48, %r7, %r148;
	xor.pred  	%p49, %p48, %p47;
	selp.b32 	%r149, 0, %r233, %p49;
	or.b32  	%r261, %r261, %r149;

$L__BB16_27:
	@%p2 bra 	$L__BB16_29;

	mov.u32 	%r248, 268435456;
	shr.u32 	%r247, %r248, %r258;
	setp.eq.s32 	%p51, %r2, 0;
	cvt.u32.u16 	%r151, %rs45;
	and.b32  	%r152, %r151, 255;
	setp.lt.s32 	%p52, %r1, %r152;
	xor.pred  	%p53, %p52, %p51;
	selp.b32 	%r153, 0, %r247, %p53;
	or.b32  	%r261, %r261, %r153;

$L__BB16_29:
	@%p6 bra 	$L__BB16_31;

	mov.u32 	%r246, 268435456;
	shr.u32 	%r245, %r246, %r258;
	setp.eq.s32 	%p55, %r4, 0;
	cvt.u32.u16 	%r154, %rs46;
	and.b32  	%r155, %r154, 255;
	setp.lt.s32 	%p56, %r3, %r155;
	xor.pred  	%p57, %p56, %p55;
	selp.b32 	%r156, 0, %r245, %p57;
	or.b32  	%r261, %r261, %r156;

$L__BB16_31:
	@%p10 bra 	$L__BB16_33;

	mov.u32 	%r244, 268435456;
	shr.u32 	%r243, %r244, %r258;
	setp.eq.s32 	%p59, %r6, 0;
	cvt.u32.u16 	%r157, %rs47;
	and.b32  	%r158, %r157, 255;
	setp.lt.s32 	%p60, %r5, %r158;
	xor.pred  	%p61, %p60, %p59;
	selp.b32 	%r159, 0, %r243, %p61;
	or.b32  	%r261, %r261, %r159;

$L__BB16_33:
	@%p14 bra 	$L__BB16_35;

	mov.u32 	%r242, 268435456;
	shr.u32 	%r241, %r242, %r258;
	setp.eq.s32 	%p63, %r8, 0;
	cvt.u32.u16 	%r160, %rs48;
	and.b32  	%r161, %r160, 255;
	setp.lt.s32 	%p64, %r7, %r161;
	xor.pred  	%p65, %p64, %p63;
	selp.b32 	%r162, 0, %r241, %p65;
	or.b32  	%r261, %r261, %r162;

$L__BB16_35:
	@%p2 bra 	$L__BB16_37;

	mov.u32 	%r256, 134217728;
	shr.u32 	%r255, %r256, %r258;
	setp.eq.s32 	%p67, %r2, 0;
	cvt.u32.u16 	%r164, %rs49;
	and.b32  	%r165, %r164, 255;
	setp.lt.s32 	%p68, %r1, %r165;
	xor.pred  	%p69, %p68, %p67;
	selp.b32 	%r166, 0, %r255, %p69;
	or.b32  	%r261, %r261, %r166;

$L__BB16_37:
	@%p6 bra 	$L__BB16_39;

	mov.u32 	%r254, 134217728;
	shr.u32 	%r253, %r254, %r258;
	setp.eq.s32 	%p71, %r4, 0;
	cvt.u32.u16 	%r167, %rs50;
	and.b32  	%r168, %r167, 255;
	setp.lt.s32 	%p72, %r3, %r168;
	xor.pred  	%p73, %p72, %p71;
	selp.b32 	%r169, 0, %r253, %p73;
	or.b32  	%r261, %r261, %r169;

$L__BB16_39:
	@%p10 bra 	$L__BB16_41;

	mov.u32 	%r252, 134217728;
	shr.u32 	%r251, %r252, %r258;
	setp.eq.s32 	%p75, %r6, 0;
	cvt.u32.u16 	%r170, %rs51;
	and.b32  	%r171, %r170, 255;
	setp.lt.s32 	%p76, %r5, %r171;
	xor.pred  	%p77, %p76, %p75;
	selp.b32 	%r172, 0, %r251, %p77;
	or.b32  	%r261, %r261, %r172;

$L__BB16_41:
	@%p14 bra 	$L__BB16_43;

	mov.u32 	%r250, 134217728;
	shr.u32 	%r249, %r250, %r258;
	setp.eq.s32 	%p79, %r8, 0;
	cvt.u32.u16 	%r173, %rs52;
	and.b32  	%r174, %r173, 255;
	setp.lt.s32 	%p80, %r7, %r174;
	xor.pred  	%p81, %p80, %p79;
	selp.b32 	%r175, 0, %r249, %p81;
	or.b32  	%r261, %r261, %r175;

$L__BB16_43:
	mov.u32 	%r176, 67108864;
	shr.u32 	%r63, %r176, %r258;
	@%p2 bra 	$L__BB16_45;

	setp.eq.s32 	%p83, %r2, 0;
	cvt.u32.u16 	%r177, %rs53;
	and.b32  	%r178, %r177, 255;
	setp.lt.s32 	%p84, %r1, %r178;
	xor.pred  	%p85, %p84, %p83;
	selp.b32 	%r179, 0, %r63, %p85;
	or.b32  	%r261, %r261, %r179;

$L__BB16_45:
	@%p6 bra 	$L__BB16_47;

	setp.eq.s32 	%p87, %r4, 0;
	cvt.u32.u16 	%r180, %rs54;
	and.b32  	%r181, %r180, 255;
	setp.lt.s32 	%p88, %r3, %r181;
	xor.pred  	%p89, %p88, %p87;
	selp.b32 	%r182, 0, %r63, %p89;
	or.b32  	%r261, %r261, %r182;

$L__BB16_47:
	@%p10 bra 	$L__BB16_49;

	setp.eq.s32 	%p91, %r6, 0;
	cvt.u32.u16 	%r183, %rs55;
	and.b32  	%r184, %r183, 255;
	setp.lt.s32 	%p92, %r5, %r184;
	xor.pred  	%p93, %p92, %p91;
	selp.b32 	%r185, 0, %r63, %p93;
	or.b32  	%r261, %r261, %r185;

$L__BB16_49:
	@%p14 bra 	$L__BB16_51;

	setp.eq.s32 	%p95, %r8, 0;
	cvt.u32.u16 	%r186, %rs56;
	and.b32  	%r187, %r186, 255;
	setp.lt.s32 	%p96, %r7, %r187;
	xor.pred  	%p97, %p96, %p95;
	selp.b32 	%r188, 0, %r63, %p97;
	or.b32  	%r261, %r261, %r188;

$L__BB16_51:
	mov.u32 	%r189, 33554432;
	shr.u32 	%r72, %r189, %r258;
	@%p2 bra 	$L__BB16_53;

	setp.eq.s32 	%p99, %r2, 0;
	cvt.u32.u16 	%r190, %rs57;
	and.b32  	%r191, %r190, 255;
	setp.lt.s32 	%p100, %r1, %r191;
	xor.pred  	%p101, %p100, %p99;
	selp.b32 	%r192, 0, %r72, %p101;
	or.b32  	%r261, %r261, %r192;

$L__BB16_53:
	@%p6 bra 	$L__BB16_55;

	setp.eq.s32 	%p103, %r4, 0;
	cvt.u32.u16 	%r193, %rs58;
	and.b32  	%r194, %r193, 255;
	setp.lt.s32 	%p104, %r3, %r194;
	xor.pred  	%p105, %p104, %p103;
	selp.b32 	%r195, 0, %r72, %p105;
	or.b32  	%r261, %r261, %r195;

$L__BB16_55:
	@%p10 bra 	$L__BB16_57;

	setp.eq.s32 	%p107, %r6, 0;
	cvt.u32.u16 	%r196, %rs59;
	and.b32  	%r197, %r196, 255;
	setp.lt.s32 	%p108, %r5, %r197;
	xor.pred  	%p109, %p108, %p107;
	selp.b32 	%r198, 0, %r72, %p109;
	or.b32  	%r261, %r261, %r198;

$L__BB16_57:
	@%p14 bra 	$L__BB16_59;

	setp.eq.s32 	%p111, %r8, 0;
	cvt.u32.u16 	%r199, %rs60;
	and.b32  	%r200, %r199, 255;
	setp.lt.s32 	%p112, %r7, %r200;
	xor.pred  	%p113, %p112, %p111;
	selp.b32 	%r201, 0, %r72, %p113;
	or.b32  	%r261, %r261, %r201;

$L__BB16_59:
	mov.u32 	%r202, 16777216;
	shr.u32 	%r81, %r202, %r258;
	@%p2 bra 	$L__BB16_61;

	setp.eq.s32 	%p115, %r2, 0;
	cvt.u32.u16 	%r203, %rs61;
	and.b32  	%r204, %r203, 255;
	setp.lt.s32 	%p116, %r1, %r204;
	xor.pred  	%p117, %p116, %p115;
	selp.b32 	%r205, 0, %r81, %p117;
	or.b32  	%r261, %r261, %r205;

$L__BB16_61:
	@%p6 bra 	$L__BB16_63;

	setp.eq.s32 	%p119, %r4, 0;
	cvt.u32.u16 	%r206, %rs62;
	and.b32  	%r207, %r206, 255;
	setp.lt.s32 	%p120, %r3, %r207;
	xor.pred  	%p121, %p120, %p119;
	selp.b32 	%r208, 0, %r81, %p121;
	or.b32  	%r261, %r261, %r208;

$L__BB16_63:
	@%p10 bra 	$L__BB16_65;

	setp.eq.s32 	%p123, %r6, 0;
	cvt.u32.u16 	%r209, %rs63;
	and.b32  	%r210, %r209, 255;
	setp.lt.s32 	%p124, %r5, %r210;
	xor.pred  	%p125, %p124, %p123;
	selp.b32 	%r211, 0, %r81, %p125;
	or.b32  	%r261, %r261, %r211;

$L__BB16_65:
	@%p14 bra 	$L__BB16_67;

	setp.eq.s32 	%p127, %r8, 0;
	cvt.u32.u16 	%r212, %rs64;
	and.b32  	%r213, %r212, 255;
	setp.lt.s32 	%p128, %r7, %r213;
	xor.pred  	%p129, %p128, %p127;
	selp.b32 	%r214, 0, %r81, %p129;
	or.b32  	%r261, %r261, %r214;

$L__BB16_67:
	add.s32 	%r259, %r259, 1;
	add.s32 	%r258, %r258, 8;
	setp.ne.s32 	%p130, %r258, 32;
	@%p130 bra 	$L__BB16_3;

	ld.param.u64 	%rd16, [kernel_ThresholdRectToPix_param_6];
	and.b64  	%rd11, %rd17, 4294967295;
	shl.b64 	%rd12, %rd17, 2;
	and.b64  	%rd13, %rd12, 17179869180;
	add.s64 	%rd14, %rd16, %rd13;
	st.global.u32 	[%rd14], %r261;
	add.s64 	%rd17, %rd11, %rd2;
	cvt.u32.u64 	%r257, %rd17;
	setp.gt.u32 	%p131, %r11, %r257;
	@%p131 bra 	$L__BB16_2;

$L__BB16_69:
	ret;

}
	// .globl	kernel_ThresholdRectToPix_OneChan
.entry kernel_ThresholdRectToPix_OneChan(
	.param .u64 .ptr .global .align 8 kernel_ThresholdRectToPix_OneChan_param_0,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_1,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_2,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_3,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_4,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_5,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_6
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<71>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<115>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd7, [kernel_ThresholdRectToPix_OneChan_param_0];
	ld.param.u32 	%r8, [kernel_ThresholdRectToPix_OneChan_param_1];
	ld.param.u32 	%r9, [kernel_ThresholdRectToPix_OneChan_param_3];
	ld.param.u64 	%rd8, [kernel_ThresholdRectToPix_OneChan_param_4];
	ld.param.u64 	%rd10, [kernel_ThresholdRectToPix_OneChan_param_5];
	ld.param.u64 	%rd9, [kernel_ThresholdRectToPix_OneChan_param_6];
	ld.global.u32 	%r1, [%rd10];
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mov.b32 	%r12, %envreg3;
	add.s32 	%r13, %r11, %r12;
	mad.lo.s32 	%r114, %r2, %r10, %r13;
	mul.lo.s32 	%r4, %r9, %r8;
	setp.le.u32 	%p1, %r4, %r114;
	@%p1 bra 	$L__BB17_5;

	cvt.s64.s32 	%rd22, %r114;
	mov.b32 	%r14, %envreg6;
	mul.lo.s32 	%r15, %r14, %r2;
	cvt.s64.s32 	%rd2, %r15;
	setp.gt.s32 	%p2, %r1, -1;
	@%p2 bra 	$L__BB17_3;
	bra.uni 	$L__BB17_2;

$L__BB17_3:
	ld.global.u32 	%r5, [%rd8];

$L__BB17_4:
	ld.param.u64 	%rd21, [kernel_ThresholdRectToPix_OneChan_param_6];
	shl.b32 	%r18, %r114, 5;
	cvt.u64.u32 	%rd15, %r18;
	add.s64 	%rd16, %rd7, %rd15;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd16+4];
	ld.global.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [%rd16];
	cvt.u32.u16 	%r19, %rs8;
	setp.lt.s32 	%p4, %r5, %r19;
	setp.eq.s32 	%p5, %r1, 0;
	xor.pred  	%p6, %p5, %p4;
	selp.b32 	%r20, 0, -2147483648, %p6;
	cvt.u32.u16 	%r21, %rs7;
	setp.lt.s32 	%p7, %r5, %r21;
	xor.pred  	%p8, %p5, %p7;
	or.b32  	%r22, %r20, 1073741824;
	selp.b32 	%r23, %r20, %r22, %p8;
	cvt.u32.u16 	%r24, %rs6;
	setp.lt.s32 	%p9, %r5, %r24;
	xor.pred  	%p10, %p5, %p9;
	or.b32  	%r25, %r23, 536870912;
	selp.b32 	%r26, %r23, %r25, %p10;
	cvt.u32.u16 	%r27, %rs5;
	setp.lt.s32 	%p11, %r5, %r27;
	xor.pred  	%p12, %p5, %p11;
	or.b32  	%r28, %r26, 268435456;
	selp.b32 	%r29, %r26, %r28, %p12;
	cvt.u32.u16 	%r30, %rs4;
	setp.lt.s32 	%p13, %r5, %r30;
	xor.pred  	%p14, %p5, %p13;
	or.b32  	%r31, %r29, 134217728;
	selp.b32 	%r32, %r29, %r31, %p14;
	cvt.u32.u16 	%r33, %rs3;
	setp.lt.s32 	%p15, %r5, %r33;
	xor.pred  	%p16, %p5, %p15;
	or.b32  	%r34, %r32, 67108864;
	selp.b32 	%r35, %r32, %r34, %p16;
	cvt.u32.u16 	%r36, %rs2;
	setp.lt.s32 	%p17, %r5, %r36;
	xor.pred  	%p18, %p5, %p17;
	or.b32  	%r37, %r35, 33554432;
	selp.b32 	%r38, %r35, %r37, %p18;
	cvt.u32.u16 	%r39, %rs1;
	setp.lt.s32 	%p19, %r5, %r39;
	xor.pred  	%p20, %p5, %p19;
	or.b32  	%r40, %r38, 16777216;
	selp.b32 	%r41, %r38, %r40, %p20;
	ld.global.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [%rd16+12];
	ld.global.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [%rd16+8];
	cvt.u32.u16 	%r42, %rs16;
	setp.lt.s32 	%p21, %r5, %r42;
	xor.pred  	%p22, %p5, %p21;
	or.b32  	%r43, %r41, 8388608;
	selp.b32 	%r44, %r41, %r43, %p22;
	cvt.u32.u16 	%r45, %rs15;
	setp.lt.s32 	%p23, %r5, %r45;
	xor.pred  	%p24, %p5, %p23;
	or.b32  	%r46, %r44, 4194304;
	selp.b32 	%r47, %r44, %r46, %p24;
	cvt.u32.u16 	%r48, %rs14;
	setp.lt.s32 	%p25, %r5, %r48;
	xor.pred  	%p26, %p5, %p25;
	or.b32  	%r49, %r47, 2097152;
	selp.b32 	%r50, %r47, %r49, %p26;
	cvt.u32.u16 	%r51, %rs13;
	setp.lt.s32 	%p27, %r5, %r51;
	xor.pred  	%p28, %p5, %p27;
	or.b32  	%r52, %r50, 1048576;
	selp.b32 	%r53, %r50, %r52, %p28;
	cvt.u32.u16 	%r54, %rs12;
	setp.lt.s32 	%p29, %r5, %r54;
	xor.pred  	%p30, %p5, %p29;
	or.b32  	%r55, %r53, 524288;
	selp.b32 	%r56, %r53, %r55, %p30;
	cvt.u32.u16 	%r57, %rs11;
	setp.lt.s32 	%p31, %r5, %r57;
	xor.pred  	%p32, %p5, %p31;
	or.b32  	%r58, %r56, 262144;
	selp.b32 	%r59, %r56, %r58, %p32;
	cvt.u32.u16 	%r60, %rs10;
	setp.lt.s32 	%p33, %r5, %r60;
	xor.pred  	%p34, %p5, %p33;
	or.b32  	%r61, %r59, 131072;
	selp.b32 	%r62, %r59, %r61, %p34;
	cvt.u32.u16 	%r63, %rs9;
	setp.lt.s32 	%p35, %r5, %r63;
	xor.pred  	%p36, %p5, %p35;
	or.b32  	%r64, %r62, 65536;
	selp.b32 	%r65, %r62, %r64, %p36;
	ld.global.v4.u8 	{%rs17, %rs18, %rs19, %rs20}, [%rd16+20];
	ld.global.v4.u8 	{%rs21, %rs22, %rs23, %rs24}, [%rd16+16];
	cvt.u32.u16 	%r66, %rs24;
	setp.lt.s32 	%p37, %r5, %r66;
	xor.pred  	%p38, %p5, %p37;
	or.b32  	%r67, %r65, 32768;
	selp.b32 	%r68, %r65, %r67, %p38;
	cvt.u32.u16 	%r69, %rs23;
	setp.lt.s32 	%p39, %r5, %r69;
	xor.pred  	%p40, %p5, %p39;
	or.b32  	%r70, %r68, 16384;
	selp.b32 	%r71, %r68, %r70, %p40;
	cvt.u32.u16 	%r72, %rs22;
	setp.lt.s32 	%p41, %r5, %r72;
	xor.pred  	%p42, %p5, %p41;
	or.b32  	%r73, %r71, 8192;
	selp.b32 	%r74, %r71, %r73, %p42;
	cvt.u32.u16 	%r75, %rs21;
	setp.lt.s32 	%p43, %r5, %r75;
	xor.pred  	%p44, %p5, %p43;
	or.b32  	%r76, %r74, 4096;
	selp.b32 	%r77, %r74, %r76, %p44;
	cvt.u32.u16 	%r78, %rs20;
	setp.lt.s32 	%p45, %r5, %r78;
	xor.pred  	%p46, %p5, %p45;
	or.b32  	%r79, %r77, 2048;
	selp.b32 	%r80, %r77, %r79, %p46;
	cvt.u32.u16 	%r81, %rs19;
	setp.lt.s32 	%p47, %r5, %r81;
	xor.pred  	%p48, %p5, %p47;
	or.b32  	%r82, %r80, 1024;
	selp.b32 	%r83, %r80, %r82, %p48;
	cvt.u32.u16 	%r84, %rs18;
	setp.lt.s32 	%p49, %r5, %r84;
	xor.pred  	%p50, %p5, %p49;
	or.b32  	%r85, %r83, 512;
	selp.b32 	%r86, %r83, %r85, %p50;
	cvt.u32.u16 	%r87, %rs17;
	setp.lt.s32 	%p51, %r5, %r87;
	xor.pred  	%p52, %p5, %p51;
	or.b32  	%r88, %r86, 256;
	selp.b32 	%r89, %r86, %r88, %p52;
	ld.global.v4.u8 	{%rs25, %rs26, %rs27, %rs28}, [%rd16+28];
	ld.global.v4.u8 	{%rs29, %rs30, %rs31, %rs32}, [%rd16+24];
	cvt.u32.u16 	%r90, %rs32;
	setp.lt.s32 	%p53, %r5, %r90;
	xor.pred  	%p54, %p5, %p53;
	or.b32  	%r91, %r89, 128;
	selp.b32 	%r92, %r89, %r91, %p54;
	cvt.u32.u16 	%r93, %rs31;
	setp.lt.s32 	%p55, %r5, %r93;
	xor.pred  	%p56, %p5, %p55;
	or.b32  	%r94, %r92, 64;
	selp.b32 	%r95, %r92, %r94, %p56;
	cvt.u32.u16 	%r96, %rs30;
	setp.lt.s32 	%p57, %r5, %r96;
	xor.pred  	%p58, %p5, %p57;
	or.b32  	%r97, %r95, 32;
	selp.b32 	%r98, %r95, %r97, %p58;
	cvt.u32.u16 	%r99, %rs29;
	setp.lt.s32 	%p59, %r5, %r99;
	xor.pred  	%p60, %p5, %p59;
	or.b32  	%r100, %r98, 16;
	selp.b32 	%r101, %r98, %r100, %p60;
	cvt.u32.u16 	%r102, %rs28;
	setp.lt.s32 	%p61, %r5, %r102;
	xor.pred  	%p62, %p5, %p61;
	or.b32  	%r103, %r101, 8;
	selp.b32 	%r104, %r101, %r103, %p62;
	cvt.u32.u16 	%r105, %rs27;
	setp.lt.s32 	%p63, %r5, %r105;
	xor.pred  	%p64, %p5, %p63;
	or.b32  	%r106, %r104, 4;
	selp.b32 	%r107, %r104, %r106, %p64;
	cvt.u32.u16 	%r108, %rs26;
	setp.lt.s32 	%p65, %r5, %r108;
	xor.pred  	%p66, %p5, %p65;
	or.b32  	%r109, %r107, 2;
	selp.b32 	%r110, %r107, %r109, %p66;
	cvt.u32.u16 	%r111, %rs25;
	setp.lt.s32 	%p67, %r5, %r111;
	xor.pred  	%p68, %p5, %p67;
	not.pred 	%p69, %p68;
	selp.u32 	%r112, 1, 0, %p69;
	or.b32  	%r113, %r110, %r112;
	and.b64  	%rd17, %rd22, 4294967295;
	shl.b64 	%rd18, %rd22, 2;
	and.b64  	%rd19, %rd18, 17179869180;
	add.s64 	%rd20, %rd21, %rd19;
	st.global.u32 	[%rd20], %r113;
	add.s64 	%rd22, %rd17, %rd2;
	cvt.u32.u64 	%r114, %rd22;
	setp.gt.u32 	%p70, %r4, %r114;
	@%p70 bra 	$L__BB17_4;
	bra.uni 	$L__BB17_5;

$L__BB17_2:
	and.b64  	%rd11, %rd22, 4294967295;
	shl.b64 	%rd12, %rd22, 2;
	and.b64  	%rd13, %rd12, 17179869180;
	add.s64 	%rd14, %rd9, %rd13;
	mov.u32 	%r16, 0;
	st.global.u32 	[%rd14], %r16;
	add.s64 	%rd22, %rd11, %rd2;
	cvt.u32.u64 	%r17, %rd22;
	setp.gt.u32 	%p3, %r4, %r17;
	@%p3 bra 	$L__BB17_2;

$L__BB17_5:
	ret;

}
.metadata_section {

.metadata 0 {
	"cl_kernel_attributes",
	"kernel_HistogramRectAllChannels",
	"reqd_work_group_size(256,1,1)"
}

.metadata 1 {
	"cl_kernel_attributes",
	"kernel_HistogramRectOneChannel",
	"reqd_work_group_size(256,1,1)"
}

.metadata 2 {
	"cl_kernel_attributes",
	"kernel_HistogramRectAllChannelsReduction",
	"reqd_work_group_size(256,1,1)"
}

.metadata 3 {
	"cl_kernel_attributes",
	"kernel_HistogramRectOneChannelReduction",
	"reqd_work_group_size(256,1,1)"
}

.metadata 4 {
	"cl_kernel_attributes",
	"kernel_ThresholdRectToPix",
	"reqd_work_group_size(256,1,1)"
}

.metadata 5 {
	"cl_kernel_attributes",
	"kernel_ThresholdRectToPix_OneChan",
	"reqd_work_group_size(256,1,1)"
}

} // end of .metadata_section

  